#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ë“¤
- test_main.pyì—ì„œ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ë“¤ì„ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬
"""

import os
import json
import time
import requests
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from hybrid_vector_manager import HybridVectorManager
from clova_embedding import ClovaEmbeddingAPI
from clova_segmentation import ClovaSegmentationClient
from news_content_extractor import NewsContentExtractor


class Data1VectorManager(HybridVectorManager):
    """data_1 ë””ë ‰í† ë¦¬ìš© ë²¡í„° ë§¤ë‹ˆì €"""
    
    def __init__(self, data_dir: str):
        """ì´ˆê¸°í™”"""
        super().__init__(data_dir)
        self.clova_embedding = ClovaEmbeddingAPI()
        self.clova_segmentation = ClovaSegmentationClient()
        self.news_extractor = NewsContentExtractor()
    
    def _csv_to_summary_text(self, csv_file: Path) -> str:
        """CSV íŒŒì¼ì„ LlamaIndex ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜"""
        try:
            df = pd.read_csv(csv_file)
            
            if df.empty:
                return f"íŒŒì¼ëª…: {csv_file.name}\në°ì´í„° ì—†ìŒ"
            
            # LlamaIndex ë°©ì‹ìœ¼ë¡œ DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text_parts = []
            
            # íŒŒì¼ëª… ì •ë³´
            text_parts.append(f"íŒŒì¼ëª…: {csv_file.name}")
            text_parts.append(f"ì´ {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
            text_parts.append("")
            
            # KRX ë°ì´í„°ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if 'ISU_ABBRV' in df.columns and 'TDD_CLSPRC' in df.columns:
                text_parts.append("=== KRX ì¼ì¼ê±°ë˜ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§) ===")
                text_parts.append("")
                
                # ì „ì²´ ë°ì´í„°ì—ì„œ ê±°ë˜ëŒ€ê¸ˆ í•˜ìœ„ 70%ì™€ ë“±ë½ë¥  í•˜ìœ„ 70%ì—ì„œ ê²¹ì¹˜ëŠ” ì£¼ì‹ ì œì™¸
                df_processed = df.copy()
                
                # FLUC_RT ì»¬ëŸ¼ì„ ë“±ë½ë¥ ë¡œ ì‚¬ìš©
                df_processed['ë“±ë½ë¥ '] = df_processed['FLUC_RT']
                df_processed['ë“±ë½ë¥ _ì ˆëŒ€ê°’'] = df_processed['ë“±ë½ë¥ '].abs()
                
                # ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸°
                df_trading_value_sorted = df_processed.sort_values('ACC_TRDVAL', ascending=True)
                bottom_70_percent_trading = int(len(df_trading_value_sorted) * 0.7)
                low_trading_stocks = set(df_trading_value_sorted.head(bottom_70_percent_trading)['ISU_ABBRV'].tolist())
                
                # ë“±ë½ë¥  ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸° (ì ˆëŒ€ê°’ ê¸°ì¤€)
                df_change_rate_sorted = df_processed.sort_values('ë“±ë½ë¥ _ì ˆëŒ€ê°’', ascending=True)
                bottom_70_percent_change = int(len(df_change_rate_sorted) * 0.7)
                low_change_stocks = set(df_change_rate_sorted.head(bottom_70_percent_change)['ISU_ABBRV'].tolist())
                
                # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ ì°¾ê¸°
                overlapping_stocks = low_trading_stocks.intersection(low_change_stocks)
                
                # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ì„ ì œì™¸í•œ ìµœì¢… í•„í„°ë§
                df_final_filtered = df_processed[~df_processed['ISU_ABBRV'].isin(overlapping_stocks)]
                
                # ê° ì¢…ëª©ë³„ë¡œ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
                for idx, row in df_final_filtered.iterrows():
                    stock_name = row.get('ISU_ABBRV', '')
                    stock_code = row.get('ISU_CD', '')
                    
                    if stock_name and stock_code:
                        # ê°€ê²© ì •ë³´
                        open_price = row.get('TDD_OPNPRC', 0)
                        high_price = row.get('TDD_HGPRC', 0)
                        low_price = row.get('TDD_LWPRC', 0)
                        close_price = row.get('TDD_CLSPRC', 0)
                        trading_value = row.get('ACC_TRDVAL', 0)
                        change_rate = row.get('FLUC_RT', 0.0)
                        
                        # ì¢…ëª© ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ êµ¬ì„±
                        stock_info = f"ì¢…ëª©ëª…: {stock_name}, ì‹œê°€: {open_price}, ê³ ê°€: {high_price}, ì €ê°€: {low_price}, ì¢…ê°€: {close_price}, ê±°ë˜ëŒ€ê¸ˆ: {trading_value}, ë“±ë½ë¥ : {change_rate:.2f}"
                        text_parts.append(stock_info)
                
                text_parts.append("")
                text_parts.append(f"ì´ {len(df_final_filtered)}ê°œ ì¢…ëª©ì˜ ê±°ë˜ ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§)")
            else:
                # ì¼ë°˜ CSV íŒŒì¼ ì²˜ë¦¬ (LlamaIndex ë°©ì‹)
                text_parts.append("ì»¬ëŸ¼ëª…:")
                for col in df.columns:
                    text_parts.append(f"  - {col}")
                text_parts.append("")
                
                # ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰)
                text_parts.append("ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰):")
                sample_df = df.head(10)
                text_parts.append(sample_df.to_string(index=False))
            
            return "\n".join(text_parts)
            
        except Exception as e:
            print(f"âŒ CSV í…ìŠ¤íŠ¸ ë³€í™˜ ì‹¤íŒ¨ ({csv_file.name}): {e}")
            return f"íŒŒì¼ëª…: {csv_file.name} (ë³€í™˜ ì‹¤íŒ¨)"
    
    def _single_article_to_text(self, article: dict, article_index: int) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ LlamaIndex ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜"""
        try:
            text_parts = []
            
            # ê¸°ì‚¬ ì œëª©
            title = article.get('title', '')
            if title:
                text_parts.append(f"ì œëª©: {title}")
            
            # ê¸°ì‚¬ ë‚´ìš© (ì„¤ëª…)
            description = article.get('description', '')
            if description:
                # HTML íƒœê·¸ ì œê±°
                import re
                clean_description = re.sub(r'<[^>]+>', '', description)
                text_parts.append(f"ë‚´ìš©: {clean_description}")
            
            # ë°œí–‰ì¼
            pub_date = article.get('pubDate', '')
            if pub_date:
                text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
            
            # ë§í¬
            link = article.get('link', '')
            if link:
                text_parts.append(f"ë§í¬: {link}")
                
                # URLì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                try:
                    full_content = self.news_extractor.extract_article_content(link)
                    if full_content and len(full_content) > 50:
                        text_parts.append(f"ì „ì²´ ë‚´ìš©: {full_content}")
                        print(f"âœ… URLì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(full_content)}ì")
                    else:
                        print(f"âš ï¸ URLì—ì„œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                except Exception as e:
                    print(f"âš ï¸ URL ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ê¸°ì‚¬ ë²ˆí˜¸
            text_parts.append(f"ê¸°ì‚¬ ë²ˆí˜¸: {article_index + 1}")
            
            return "\n".join(text_parts)
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return f"ë‰´ìŠ¤ ê¸°ì‚¬ {article_index + 1}: ì²˜ë¦¬ ì˜¤ë¥˜"
    
    def process_documents(self) -> bool:
        """ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìƒì„±"""
        try:
            print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
            
            # ë””ë ‰í† ë¦¬ í™•ì¸
            data_path = Path(self.data_dir)
            if not data_path.exists():
                print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_dir}")
                return False
            
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            csv_files = list(data_path.glob("*.csv"))
            json_files = list(data_path.glob("*.json"))
            
            print(f"ğŸ“Š ë°œê²¬ëœ íŒŒì¼:")
            print(f"  CSV íŒŒì¼: {len(csv_files)}ê°œ")
            print(f"  JSON íŒŒì¼: {len(json_files)}ê°œ")
            
            # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
            metadata = []
            vectors = []
            
            # CSV íŒŒì¼ ì²˜ë¦¬
            for csv_file in csv_files:
                try:
                    print(f"\nğŸ“ˆ CSV íŒŒì¼ ì²˜ë¦¬: {csv_file.name}")
                    
                    # CSVë¥¼ ìš”ì•½ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    summary_text = self._csv_to_summary_text(csv_file)
                    print(f"   ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(summary_text)}ì")
                    
                    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                    chunks = self._split_text(summary_text)
                    print(f"   ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
                    
                    # ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
                    for i, chunk in enumerate(chunks):
                        try:
                            vector = self.clova_embedding.get_embedding(chunk)
                            if vector is not None:
                                vectors.append(vector)
                                metadata.append({
                                    'filename': csv_file.name,
                                    'chunk_index': i,
                                    'text_content': chunk,
                                    'file_type': 'csv',
                                    'stock_name': csv_file.stem
                                })
                                print(f"   âœ… ì²­í¬ {i+1} ë²¡í„° ìƒì„± ì™„ë£Œ")
                            else:
                                print(f"   âŒ ì²­í¬ {i+1} ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                        except Exception as e:
                            print(f"   âŒ ì²­í¬ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    
                except Exception as e:
                    print(f"âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({csv_file.name}): {e}")
            
            # JSON íŒŒì¼ ì²˜ë¦¬ (ë‰´ìŠ¤ ë°ì´í„°)
            for json_file in json_files:
                try:
                    print(f"\nğŸ“° JSON íŒŒì¼ ì²˜ë¦¬: {json_file.name}")
                    
                    with open(json_file, 'r', encoding='utf-8') as f:
                        news_data = json.load(f)
                    
                    # ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ê°œë³„ ê¸°ì‚¬ ì¶”ì¶œ
                    all_articles = []
                    for stock_name, stock_data in news_data.get('stocks', {}).items():
                        for item in stock_data.get('items', []):
                            item['stock_name'] = stock_name
                            all_articles.append(item)
                    
                    print(f"   ì´ ë‰´ìŠ¤ ê¸°ì‚¬: {len(all_articles)}ê°œ")
                    
                    # ê° ê¸°ì‚¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
                    for article_index, article in enumerate(all_articles):
                        try:
                            # ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            article_text = self._single_article_to_text(article, article_index)
                            print(f"   ê¸°ì‚¬ {article_index + 1} í…ìŠ¤íŠ¸ ê¸¸ì´: {len(article_text)}ì")
                            
                            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                            chunks = self._split_text(article_text)
                            print(f"   ê¸°ì‚¬ {article_index + 1} ì²­í¬: {len(chunks)}ê°œ")
                            
                            # ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
                            for chunk_index, chunk in enumerate(chunks):
                                try:
                                    vector = self.clova_embedding.get_embedding(chunk)
                                    if vector is not None:
                                        vectors.append(vector)
                                        metadata.append({
                                            'filename': json_file.name,
                                            'chunk_index': chunk_index,
                                            'text_content': chunk,
                                            'file_type': 'json',
                                            'article_index': article_index,
                                            'stock_name': article.get('stock_name', 'Unknown')
                                        })
                                        print(f"   âœ… ê¸°ì‚¬ {article_index + 1} ì²­í¬ {chunk_index + 1} ë²¡í„° ìƒì„± ì™„ë£Œ")
                                    else:
                                        print(f"   âŒ ê¸°ì‚¬ {article_index + 1} ì²­í¬ {chunk_index + 1} ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                                except Exception as e:
                                    print(f"   âŒ ê¸°ì‚¬ {article_index + 1} ì²­í¬ {chunk_index + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            
                        except Exception as e:
                            print(f"   âŒ ê¸°ì‚¬ {article_index + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    
                except Exception as e:
                    print(f"âŒ JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({json_file.name}): {e}")
            
            # ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
            if vectors and metadata:
                print(f"\nğŸ’¾ ë²¡í„° ë° ë©”íƒ€ë°ì´í„° ì €ì¥")
                print(f"   ì´ ë²¡í„°: {len(vectors)}ê°œ")
                print(f"   ì´ ë©”íƒ€ë°ì´í„°: {len(metadata)}ê°œ")
                
                # ë²¡í„° ì €ì¥
                import numpy as np
                vectors_array = np.array(vectors)
                
                # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
                current_dir = Path(__file__).parent
                vector_db_1_dir = current_dir.parent / "vector_db_1"
                vector_db_1_dir.mkdir(exist_ok=True)
                
                # FAISS ì¸ë±ìŠ¤ ì €ì¥
                import faiss
                dimension = vectors_array.shape[1]
                index = faiss.IndexFlatIP(dimension)
                index.add(vectors_array.astype('float32'))
                
                faiss.write_index(index, str(vector_db_1_dir / "hybrid_vectors.faiss"))
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                with open(vector_db_1_dir / "hybrid_metadata.json", 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                print("âœ… ë²¡í„° ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print("âŒ ì €ì¥í•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False


class VectorDBAnalyzer:
    """Vector DB ê¸°ë°˜ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.api_base_url = "http://localhost:8000"
        self.clova_client = self._create_clova_client()
    
    def _create_clova_client(self):
        """CLOVA í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        class CompletionExecutor:
            def __init__(self, host, api_key, request_id, model_endpoint=None):
                self.host = host
                self.api_key = api_key
                self.request_id = request_id
                self.model_endpoint = model_endpoint or "/v3/tasks/yl1fvofj/chat-completions"
            
            def _send_request(self, completion_request):
                """API ìš”ì²­ ì „ì†¡"""
                import http.client
                import json
                
                request_message = json.dumps(completion_request)
                
                conn = http.client.HTTPSConnection(self.host)
                headers = {
                    'Content-Type': 'application/json',
                    'X-NCP-CLOVASTUDIO-API-KEY': self.api_key,
                    'X-NCP-APIGW-API-KEY': self.api_key
                }
                
                conn.request('POST', self.model_endpoint, request_message, headers)
                response = conn.getresponse()
                result = json.loads(response.read().decode(encoding='utf-8'))
                conn.close()
                return result
            
            def execute(self, completion_request):
                """ì‹¤í–‰"""
                try:
                    res = self._send_request(completion_request)
                    return res
                except Exception as e:
                    print(f'Exception: {e}')
                    return None
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        host = "clovastudio.apigw.ntruss.com"
        api_key = os.getenv("CLOVA_API_KEY")
        request_id = os.getenv("CLOVA_REQUEST_ID")
        
        return CompletionExecutor(host, api_key, request_id)
    
    def check_faiss_api_server(self) -> bool:
        """FAISS API ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.api_base_url}/health", timeout=10)
            return response.status_code == 200
        except Exception as e:
            print(f"âŒ FAISS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def search_vectors(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰"""
        try:
            response = requests.post(
                f"{self.api_base_url}/search",
                json={"query": query, "top_k": top_k},
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()["results"]
            else:
                print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_vectors(self) -> str:
        """ë²¡í„° ë¶„ì„"""
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            query = "ì§€ë‚œ 24ì‹œê°„ì˜ êµ­ë‚´ ì£¼ì‹ ì‹œì¥ì—ì„œ ê°€ì¥ ì´ìŠˆê°€ ëœ ì£¼ì‹ ì¢…ëª© 3ê°œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            results = self.search_vectors(query, top_k=15)
            
            if not results:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            text_contents = [result["text_content"] for result in results]
            
            # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_analysis_prompt(text_contents)
            
            # CLOVA API í˜¸ì¶œ
            completion_request = {
                'messages': [{'role': 'user', 'content': prompt}],
                'topP': 0.8,
                'temperature': 0.3,
                'maxTokens': 4000
            }
            
            response = self.clova_client.execute(completion_request)
            
            if response and 'choices' in response:
                return response['choices'][0]['message']['content']
            else:
                return "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def _create_analysis_prompt(self, text_contents: List[str]) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        combined_text = "\n\n".join(text_contents)
        
        prompt = f"""
ë‹¤ìŒì€ êµ­ë‚´ ì£¼ì‹ ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ì™€ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§€ë‚œ 24ì‹œê°„ì˜ êµ­ë‚´ ì£¼ì‹ ì‹œì¥ì—ì„œ ê°€ì¥ ì´ìŠˆê°€ ëœ ì£¼ì‹ ì¢…ëª© 3ê°œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì¤‘ìš”: ì¸ì‚¬ë§ì´ë‚˜ ì„œë¡  ì—†ì´ ë°”ë¡œ ë¶„ì„ ë‚´ìš©ì„ ì‹œì‘í•˜ì„¸ìš”.

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. ê°€ì¥ ì´ìŠˆê°€ ëœ ì£¼ì‹ ì¢…ëª© 3ê°œë¥¼ ì„ ì •
2. ê° ì¢…ëª©ë³„ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
   - ì¢…ëª©ëª…ê³¼ ì¢…ëª©ì½”ë“œ
   - ì£¼ê°€ ë³€ë™ ì¶”ì´
   - ì£¼ìš” ë‰´ìŠ¤ ë° ì´ìŠˆ
   - íˆ¬ì í¬ì¸íŠ¸
   - ì£¼ì˜ì‚¬í•­

ë¶„ì„ í˜•ì‹:
## ğŸ“Š ì§€ë‚œ 24ì‹œê°„ êµ­ë‚´ ì£¼ì‹ ì‹œì¥ ì´ìŠˆ ì¢…ëª© ë¶„ì„

### 1. [ì¢…ëª©ëª…] (ì¢…ëª©ì½”ë“œ)
- **ì£¼ê°€ ë³€ë™**: [ë³€ë™ ë‚´ìš©]
- **ì£¼ìš” ë‰´ìŠ¤**: [ë‰´ìŠ¤ ìš”ì•½]
- **íˆ¬ì í¬ì¸íŠ¸**: [íˆ¬ì ê´€ì ]
- **ì£¼ì˜ì‚¬í•­**: [ë¦¬ìŠ¤í¬ ìš”ì†Œ]

### 2. [ì¢…ëª©ëª…] (ì¢…ëª©ì½”ë“œ)
[ë™ì¼í•œ í˜•ì‹]

### 3. [ì¢…ëª©ëª…] (ì¢…ëª©ì½”ë“œ)
[ë™ì¼í•œ í˜•ì‹]

## ğŸ“ˆ ì¢…í•© ë¶„ì„
[ì „ì²´ì ì¸ ì‹œì¥ ë™í–¥ ë° íˆ¬ìì ê´€ì ì—ì„œì˜ ë¶„ì„]

---

ì°¸ê³  ë°ì´í„°:
{combined_text}
"""
        
        return prompt
    
    def save_report(self, report: str) -> str:
        """ë³´ê³ ì„œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
            current_dir = Path(__file__).parent
            report_file = current_dir.parent / "data_2" / f"vector_db_report_{timestamp}.txt"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"ğŸ’¾ Vector DB ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_file}")
            return report_file
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def run_analysis(self) -> bool:
        """ë¶„ì„ ì‹¤í–‰"""
        try:
            # FAISS API ì„œë²„ í™•ì¸
            if not self.check_faiss_api_server():
                print("âŒ FAISS API ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            # ë²¡í„° ë¶„ì„
            analysis_result = self.analyze_vectors()
            
            if analysis_result:
                # ë³´ê³ ì„œ ì €ì¥
                report_file = self.save_report(analysis_result)
                if report_file:
                    print("âœ… Vector DB ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ")
                    return True
                else:
                    print("âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"âŒ Vector DB ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False 