#!/usr/bin/env python3
"""
í†µí•© ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- KRX ì¼ì¼ê±°ë˜ì •ë³´ ìˆ˜ì§‘
- ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
- CLOVA ë¶„ì„ ë° ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ
- Function Callingì„ í†µí•œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
"""

from datetime import datetime
import subprocess
import time
import requests
import json
from pathlib import Path
from typing import List, Dict, Any
import os
from krx_api_client import KRXAPIClient
from naver_news_client import NaverNewsClient
from faiss_data_analyzer import FAISSDataAnalyzer
from stock_extractor import StockExtractor
from stock_news_collector import StockNewsCollector
from hybrid_vector_manager import HybridVectorManager
from clova_embedding import ClovaEmbeddingAPI
from clova_segmentation import ClovaSegmentationClient
from news_content_extractor import NewsContentExtractor

def start_faiss_api_server():
    """FAISS API ì„œë²„ ì‹œì‘"""
    print("\n" + "=" * 60)
    print("ğŸš€ FAISS API ì„œë²„ ì‹œì‘")
    print("=" * 60)
    
    # ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        response = requests.get("http://localhost:8000/health", timeout=3)
        if response.status_code == 200:
            print("âœ… FAISS API ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return True
    except:
        pass
    
    # ì„œë²„ ì‹œì‘
    try:
        print("ğŸ”§ FAISS API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        process = subprocess.Popen(
            ["python", "faiss_vector_api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
        for i in range(30):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
            time.sleep(1)
            try:
                response = requests.get("http://localhost:8000/health", timeout=2)
                if response.status_code == 200:
                    print("âœ… FAISS API ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                    return True
            except:
                continue
        
        print("âŒ FAISS API ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
        return False
        
    except Exception as e:
        print(f"âŒ FAISS API ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def start_vector_db1_api_server():
    """Vector DB1 FAISS API ì„œë²„ ì‹œì‘"""
    print("\n" + "=" * 60)
    print("ğŸš€ Vector DB1 FAISS API ì„œë²„ ì‹œì‘")
    print("=" * 60)
    
    # ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        response = requests.get("http://localhost:8001/health", timeout=3)
        if response.status_code == 200:
            print("âœ… Vector DB1 FAISS API ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return True
    except:
        pass
    
    # ì„œë²„ ì‹œì‘
    try:
        print("ğŸ”§ Vector DB1 FAISS API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        process = subprocess.Popen(
            ["python", "faiss_vector_db1_api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
        for i in range(30):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
            time.sleep(1)
            try:
                response = requests.get("http://localhost:8001/health", timeout=2)
                if response.status_code == 200:
                    print("âœ… Vector DB1 FAISS API ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                    return True
            except:
                continue
        
        print("âŒ Vector DB1 FAISS API ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
        return False
        
    except Exception as e:
        print(f"âŒ Vector DB1 FAISS API ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ===== ì„¤ì • =====
    # True: ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ìˆ˜í–‰
    # False: ê¸°ì¡´ ì €ì¥ëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ìˆ˜í–‰
    ENABLE_DATA_COLLECTION = True  # ë°ì´í„° ìˆ˜ì§‘ ê¸°ëŠ¥ on/off
    
    print("=" * 60)
    print("ğŸš€ í†µí•© ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("ğŸ“… ì‹¤í–‰ ì‹œê°„:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    print("ğŸ”§ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ:", "ON" if ENABLE_DATA_COLLECTION else "OFF")
    print("=" * 60)
    
    # ===== ë‰´ìŠ¤ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì • =====
    # ìµœì‹ ìˆœìœ¼ë¡œ 30ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘
    NEWS_CONFIG = {
        "query": "êµ­ë‚´ ì£¼ì‹ ì£¼ê°€",  # ê²€ìƒ‰ì–´
        "display": 100,           # ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜ (ìµœëŒ€ 100)
        "start": 1,               # ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜
        "sort": "date",           # ì •ë ¬ ë°©ë²• ("sim": ê´€ë ¨ë„ìˆœ, "date": ë‚ ì§œìˆœ)
        "filter_by_date": True,   # ë‚ ì§œ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
        "days_back": 1,           # ì§€ë‚œ ëª‡ ì¼ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ì§€ (1 = ì§€ë‚œ 1ì¼)
        "target_count": 30        # í•„í„°ë§ í›„ ëª©í‘œ ë‰´ìŠ¤ ê°œìˆ˜ (ìµœì‹ ìˆœ 30ê°œ)
    }
    
    print(f"ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ì„¤ì •: {NEWS_CONFIG}")
    
    # ===== ë°ì´í„° ìˆ˜ì§‘ (ì¡°ê±´ë¶€ ì‹¤í–‰) =====
    if ENABLE_DATA_COLLECTION:
        print("\n" + "=" * 60)
        print("ğŸ“Š ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        # 1. KRX ì¼ì¼ê±°ë˜ì •ë³´ ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("ğŸ“ˆ KRX ì¼ì¼ê±°ë˜ì •ë³´ ìˆ˜ì§‘")
        print("=" * 60)
        
        krx_client = KRXAPIClient()
        krx_filename = krx_client.collect_and_save_daily_data()
        
        # 2. ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘")
        print("=" * 60)
        
        news_client = NaverNewsClient()
        
        # API ì •ë³´ í™•ì¸
        api_info = news_client.get_api_info()
        if not api_info['client_id_set'] or not api_info['client_secret_set']:
            print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ .env íŒŒì¼ì— ì„¤ì •í•˜ì„¸ìš”.")
            news_filename = None
        else:
            print("âœ… ë„¤ì´ë²„ API í‚¤ ì„¤ì • ì™„ë£Œ")
            print(f"ğŸ” '{NEWS_CONFIG['query']}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            
            # ì‚¬ìš©ì ì •ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
            success = news_client.get_custom_news(
                query=NEWS_CONFIG["query"],
                display=NEWS_CONFIG["display"],
                start=NEWS_CONFIG["start"],
                sort=NEWS_CONFIG["sort"],
                filter_by_date=NEWS_CONFIG["filter_by_date"],
                days_back=NEWS_CONFIG["days_back"],
                target_count=NEWS_CONFIG["target_count"]
            )
            
            if success:
                print("âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                # íŒŒì¼ëª…ì€ naver_news_clientì—ì„œ ìë™ ìƒì„±ë¨
                news_filename = "ìˆ˜ì§‘ ì™„ë£Œ"
            else:
                print("âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
                news_filename = None
        
        # 3. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ“‹ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        if krx_filename:
            print(f"âœ… KRX ë°ì´í„°: {krx_filename}")
        else:
            print("âŒ KRX ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        if news_filename:
            print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤: {news_filename}")
        else:
            print("âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
    else:
        print("\n" + "=" * 60)
        print("ğŸ“Š ê¸°ì¡´ ì €ì¥ëœ ë°ì´í„° ì‚¬ìš©")
        print("=" * 60)
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸° - ê¸°ì¡´ íŒŒì¼ë“¤ ì‚¬ìš©")
    
    # 4. FAISS API ì„œë²„ ì‹œì‘
    if not start_faiss_api_server():
        print("âŒ FAISS API ì„œë²„ ì‹œì‘ ì‹¤íŒ¨ë¡œ ì¸í•´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # 5. CLOVA ë¶„ì„ ë° ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ
    print("\n" + "=" * 60)
    print("ğŸ¤– CLOVA ë¶„ì„ ë° ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ")
    print("=" * 60)
    
    # FAISS ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = FAISSDataAnalyzer()
    
    # ì²« ë²ˆì§¸ ë°ì´í„° ë³€í™˜ ê²°ê³¼ í™•ì¸
    print("\n" + "=" * 60)
    print("ğŸ” ë°ì´í„° ë³€í™˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    print("=" * 60)
    
    # KRX ë°ì´í„° ì²« ë²ˆì§¸ í•­ëª© ë³€í™˜ ê²°ê³¼ í™•ì¸
    print("\nğŸ“Š KRX ë°ì´í„° ì²« ë²ˆì§¸ í•­ëª© ë³€í™˜ ê²°ê³¼:")
    print("-" * 40)
    
    # KRX íŒŒì¼ ì°¾ê¸°
    import glob
    krx_files = glob.glob("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data/krx_daily_trading_*.csv")
    if krx_files:
        import pandas as pd
        df = pd.read_csv(krx_files[0])
        print(f"ğŸ“„ íŒŒì¼: {krx_files[0]}")
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        
        # ì²« ë²ˆì§¸ ì¢…ëª© ì •ë³´ ì¶œë ¥
        if len(df) > 0:
            first_row = df.iloc[0]
            print(f"\nğŸ¢ ì²« ë²ˆì§¸ ì¢…ëª© ì •ë³´:")
            print(f"  ì¢…ëª©ëª…: {first_row.get('ISU_ABBRV', 'N/A')}")
            print(f"  ì¢…ëª©ì½”ë“œ: {first_row.get('ISU_CD', 'N/A')}")
            print(f"  ì‹œê°€: {first_row.get('TDD_OPNPRC', 'N/A')}")
            print(f"  ê³ ê°€: {first_row.get('TDD_HGPRC', 'N/A')}")
            print(f"  ì €ê°€: {first_row.get('TDD_LWPRC', 'N/A')}")
            print(f"  ì¢…ê°€: {first_row.get('TDD_CLSPRC', 'N/A')}")
            print(f"  ê±°ë˜ëŒ€ê¸ˆ: {first_row.get('ACC_TRDVAL', 'N/A')}")
            print(f"  ë“±ë½ë¥ : {first_row.get('FLUC_RT', 'N/A')}")
    
    # ë‰´ìŠ¤ ë°ì´í„° ì²« ë²ˆì§¸ í•­ëª© ë³€í™˜ ê²°ê³¼ í™•ì¸
    print("\nğŸ“° ë‰´ìŠ¤ ë°ì´í„° ì²« ë²ˆì§¸ í•­ëª© ë³€í™˜ ê²°ê³¼:")
    print("-" * 40)
    
    # ë‰´ìŠ¤ íŒŒì¼ ì°¾ê¸°
    news_files = glob.glob("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data/naver_news_*.json")
    if news_files:
        import json
        with open(news_files[0], 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        
        print(f"ğŸ“„ íŒŒì¼: {news_files[0]}")
        articles = news_data.get('items', [])
        print(f"ğŸ“Š ì „ì²´ ë‰´ìŠ¤: {len(articles)}ê°œ")
        
        if len(articles) > 0:
            first_article = articles[0]
            print(f"\nğŸ“° ì²« ë²ˆì§¸ ë‰´ìŠ¤ ì •ë³´:")
            print(f"  ì œëª©: {first_article.get('title', 'N/A')}")
            print(f"  ë‚´ìš©: {first_article.get('description', 'N/A')[:100]}...")
            print(f"  ë°œí–‰ì¼: {first_article.get('pubDate', 'N/A')}")
    
    print("\n" + "=" * 60)
    
    print("ğŸ” FAISS ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë¶„ì„ ì‹œì‘...")
    # ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œì— ë”°ë¼ ë²¡í„° ì¬êµ¬ì¶• ì—¬ë¶€ ê²°ì •
    rebuild_vectors = ENABLE_DATA_COLLECTION  # ë°ì´í„° ìˆ˜ì§‘ ì‹œì—ë§Œ ë²¡í„° ì¬êµ¬ì¶•
    analysis_success = analyzer.run_analysis(rebuild_vectors=rebuild_vectors)
    
    if analysis_success:
        print("âœ… FAISS ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ")
        
        # ===== ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ ë° ë°ì´í„° ìˆ˜ì§‘ (ì¡°ê±´ë¶€ ì‹¤í–‰) =====
        if ENABLE_DATA_COLLECTION:
            # ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ
            print("\nğŸ“Š ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ ì‹œì‘...")
            extractor = StockExtractor()
            extraction_success = extractor.run_extraction()
            
            if extraction_success:
                print("âœ… ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ ì™„ë£Œ")
                
                # 5. ìë™ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ (ì¶”ì¶œëœ ì¢…ëª©ëª… ì‚¬ìš©)
                print("\n" + "=" * 60)
                print("ğŸ“ˆ ìë™ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘")
                print("=" * 60)
                
                # stock_extractorì—ì„œ ì¶”ì¶œëœ ì¢…ëª©ëª…ì„ ê°€ì ¸ì™€ì„œ ìë™ ìˆ˜ì§‘
                extracted_stocks = extractor.get_extracted_stocks()
                
                if extracted_stocks:
                    from stock_data_collector import StockDataCollector
                    stock_collector = StockDataCollector()
                    auto_collection_success = stock_collector.run_auto_collection(extracted_stocks)
                    
                    if auto_collection_success:
                        print("âœ… ìë™ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                    else:
                        print("âŒ ìë™ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                else:
                    print("âŒ ì¶”ì¶œëœ ì¢…ëª©ëª…ì´ ì—†ì–´ ìë™ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                
                # 6. ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘
                print("\n" + "=" * 60)
                print("ğŸ“° ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘")
                print("=" * 60)
                
                news_stocks = extracted_stocks
                news_collector = StockNewsCollector()
                news_success = news_collector.run_collection(news_stocks)
                
                if news_success:
                    print("âœ… ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    print("âŒ ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            else:
                print("âŒ ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ ë° ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        else:
            print("âœ… ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ OFF - ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ ë° ë°ì´í„° ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸°")
    else:
        print("âŒ FAISS ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨")
    
    # ===== data_1 í´ë” ì„ë² ë”© (ì¡°ê±´ë¶€ ì‹¤í–‰) =====
    if ENABLE_DATA_COLLECTION:
        print("\n" + "=" * 60)
        print("ğŸ” data_1 í´ë” íŒŒì¼ë“¤ ì„ë² ë”©")
        print("=" * 60)
        
        try:
            # vector_db_1 ë””ë ‰í† ë¦¬ ìƒì„±
            vector_db_1_path = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/vector_db_1")
            vector_db_1_path.mkdir(exist_ok=True)
            
            # data_1 í´ë”ìš© ë²¡í„° ë§¤ë‹ˆì € ìƒì„±
            class Data1VectorManager(HybridVectorManager):
                def __init__(self, data_dir: str):
                    self.data_dir = Path(data_dir)
                    self.vector_dir = vector_db_1_path
                    self.vector_dir.mkdir(exist_ok=True)
                    
                    # CLOVA í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                    self.embedding_client = ClovaEmbeddingAPI()
                    self.segmentation_client = ClovaSegmentationClient()
                    self.news_extractor = NewsContentExtractor()
                    
                    # ë²¡í„° ì €ì¥ì†Œ
                    self.vectors_file = self.vector_dir / "hybrid_vectors.pkl"
                    self.metadata_file = self.vector_dir / "hybrid_metadata.json"
                    
                    # ë²¡í„° ë°ì´í„° ë¡œë“œ
                    self.vectors = []
                    self.metadata = []
                    self._load_vectors()
                
                def _csv_to_summary_text(self, csv_file: Path) -> str:
                    """CSV íŒŒì¼ì„ ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜"""
                    try:
                        import pandas as pd
                        from datetime import datetime, timedelta
                        
                        # CSV íŒŒì¼ ì½ê¸°
                        df = pd.read_csv(csv_file)
                        
                        # Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
                        df['Date'] = pd.to_datetime(df['Date'])
                        
                        # ì¢…ëª©ëª… ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                        filename = csv_file.name
                        stock_name = filename.split('_')[0]  # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ ì¢…ëª©ëª…
                        
                        # í˜„ì¬ ë‚ ì§œ (ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
                        latest_date = df['Date'].max()
                        
                        # ì „ì¼ ë°ì´í„°
                        yesterday_data = df.iloc[-2] if len(df) > 1 else df.iloc[-1]
                        
                        # ì§€ë‚œ ì¼ì£¼ì¼ ë°ì´í„° (ìµœê·¼ 7ì¼)
                        week_ago = latest_date - timedelta(days=7)
                        week_data = df[df['Date'] >= week_ago]
                        
                        # ì§€ë‚œ 3ê°œì›” ë°ì´í„° (ìµœê·¼ 90ì¼)
                        three_months_ago = latest_date - timedelta(days=90)
                        three_months_data = df[df['Date'] >= three_months_ago]
                        
                        # ì§€ë‚œ 6ê°œì›” ë°ì´í„° (ì „ì²´ ë°ì´í„°)
                        six_months_data = df
                        
                        # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
                        summary_parts = []
                        summary_parts.append(f"ì¢…ëª©: {stock_name}")
                        summary_parts.append("")
                        
                        # ì „ì¼ ë°ì´í„°
                        summary_parts.append(f"ì „ì¼ ê³ ê°€: {yesterday_data['High']:,.0f}, ì €ê°€: {yesterday_data['Low']:,.0f}, ì‹œê°€: {yesterday_data['Open']:,.0f}, ì¢…ê°€: {yesterday_data['Close']:,.0f}, ê±°ë˜ëŸ‰: {yesterday_data['Volume']:,.0f}, ë“±ë½ë¥ : {yesterday_data['Change']:.2f}%")
                        
                        # ì§€ë‚œ ì¼ì£¼ì¼ ë°ì´í„°
                        if len(week_data) > 0:
                            week_high = week_data['High'].max()
                            week_low = week_data['Low'].min()
                            week_change = ((week_data['Close'].iloc[-1] - week_data['Open'].iloc[0]) / week_data['Open'].iloc[0]) * 100
                            summary_parts.append(f"ì§€ë‚œ ì¼ì£¼ì¼ ê³ ê°€: {week_high:,.0f}, ì €ê°€: {week_low:,.0f}, ë“±ë½ë¥ : {week_change:.2f}%")
                        else:
                            summary_parts.append("ì§€ë‚œ ì¼ì£¼ì¼ ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                        
                        # ì§€ë‚œ 3ê°œì›” ë°ì´í„°
                        if len(three_months_data) > 0:
                            three_months_high = three_months_data['High'].max()
                            three_months_low = three_months_data['Low'].min()
                            three_months_change = ((three_months_data['Close'].iloc[-1] - three_months_data['Open'].iloc[0]) / three_months_data['Open'].iloc[0]) * 100
                            summary_parts.append(f"ì§€ë‚œ 3ê°œì›” ê³ ê°€: {three_months_high:,.0f}, ì €ê°€: {three_months_low:,.0f}, ë“±ë½ë¥ : {three_months_change:.2f}%")
                        else:
                            summary_parts.append("ì§€ë‚œ 3ê°œì›” ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                        
                        # ì§€ë‚œ 6ê°œì›” ë°ì´í„°
                        if len(six_months_data) > 0:
                            six_months_high = six_months_data['High'].max()
                            six_months_low = six_months_data['Low'].min()
                            six_months_change = ((six_months_data['Close'].iloc[-1] - six_months_data['Open'].iloc[0]) / six_months_data['Open'].iloc[0]) * 100
                            summary_parts.append(f"ì§€ë‚œ 6ê°œì›” ê³ ê°€: {six_months_high:,.0f}, ì €ê°€: {six_months_low:,.0f}, ë“±ë½ë¥ : {six_months_change:.2f}%")
                        else:
                            summary_parts.append("ì§€ë‚œ 6ê°œì›” ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                        
                        return "\n".join(summary_parts)
                        
                    except Exception as e:
                        print(f"    âŒ CSV ìš”ì•½ ë³€í™˜ ì‹¤íŒ¨: {csv_file.name} - {e}")
                        return f"íŒŒì¼ëª…: {csv_file.name} (ìš”ì•½ ë³€í™˜ ì‹¤íŒ¨)"
                
                def _single_article_to_text(self, article: dict, article_index: int) -> str:
                    """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (vector_db ë°©ì‹) - URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ"""
                    text_parts = []
                    
                    # ê¸°ì‚¬ ì œëª©
                    title = article.get('title', '')
                    if title:
                        # HTML íƒœê·¸ ì œê±°
                        import re
                        clean_title = re.sub(r'<[^>]+>', '', title)
                        text_parts.append(f"ì œëª©: {clean_title}")
                    
                    # URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                    full_content = None
                    url = article.get('originallink') or article.get('link')
                    
                    if url:
                        try:
                            full_content = self.news_extractor.extract_article_content(url)
                            if full_content and full_content.get('content'):
                                print(f"      âœ… ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(full_content['content'])}ì")
                            else:
                                print(f"      âš ï¸ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨, ìš”ì•½ë¬¸ ì‚¬ìš©")
                        except Exception as e:
                            print(f"      âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                    
                    # ì „ì²´ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìš”ì•½ë¬¸ ì‚¬ìš©
                    if full_content and full_content.get('content'):
                        content = full_content['content']
                        # HTML íƒœê·¸ ì œê±°
                        import re
                        clean_content = re.sub(r'<[^>]+>', '', content)
                        text_parts.append(f"ë³¸ë¬¸: {clean_content}")
                    else:
                        # ìš”ì•½ë¬¸ ì‚¬ìš©
                        description = article.get('description', '')
                        if description:
                            # HTML íƒœê·¸ ì œê±°
                            import re
                            clean_description = re.sub(r'<[^>]+>', '', description)
                            # ... ë¶€ë¶„ ì œê±°
                            clean_description = clean_description.replace('...', '').strip()
                            text_parts.append(f"ë‚´ìš©: {clean_description}")
                    
                    # ë°œí–‰ì¼
                    pub_date = article.get('pubDate', '')
                    if pub_date:
                        text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
                    
                    final_text = "\n".join(text_parts)
                    return final_text
            
            # data_1 í´ë”ìš© ë²¡í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
            vector_manager = Data1VectorManager(
                data_dir="/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1"
            )
            
            print("ğŸ”§ data_1 í´ë” ë²¡í„° ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {vector_manager.data_dir}")
            print(f"ğŸ“ ë²¡í„° ë””ë ‰í† ë¦¬: {vector_manager.vector_dir}")
            
            # data_1 í´ë”ì˜ íŒŒì¼ë“¤ ì²˜ë¦¬
            print("\nğŸ“Š data_1 í´ë” íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
            new_metadata = []
            
            # CSV íŒŒì¼ ì²˜ë¦¬ - ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜
            csv_files = list(Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1").glob("*.csv"))
            print(f"  ğŸ“Š CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
            for csv_file in csv_files:
                try:
                    print(f"    ğŸ“Š CSV íŒŒì¼ ì²˜ë¦¬: {csv_file.name}")
                    # ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜
                    summary_text = vector_manager._csv_to_summary_text(csv_file)
                    print(f"      ğŸ“ ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(summary_text)}ì")
                    print(f"      ğŸ“ ìš”ì•½ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {summary_text[:200]}...")
                    
                    # ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹ (ë³´í†µ 1ê°œ ì²­í¬ê°€ ë  ê²ƒ)
                    chunks = vector_manager._segment_text_with_clova(summary_text, max_length=2048)
                    print(f"      ğŸ“ ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                    
                    for i, chunk in enumerate(chunks):
                        metadata_entry = {
                            "filename": csv_file.name,
                            "type": "csv",
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "text_content": chunk,
                            "text_length": len(chunk),
                            "created_at": "2025-01-31T00:00:00"
                        }
                        new_metadata.append(metadata_entry)
                        print(f"      ğŸ“ ì²­í¬ {i+1}: {len(chunk)}ì")
                        
                except Exception as e:
                    print(f"    âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # JSON íŒŒì¼ ì²˜ë¦¬ (ë‰´ìŠ¤ ë°ì´í„°) - ê°œë³„ ê¸°ì‚¬ë³„ ì²˜ë¦¬
            json_files = list(Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1").glob("*.json"))
            print(f"  ğŸ“„ JSON íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
            
            for json_file in json_files:
                print(f"    ğŸ” JSON íŒŒì¼ ì²˜ë¦¬: {json_file.name}")
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        news_data = json.load(f)
                    
                    # stocks êµ¬ì¡°ì—ì„œ ê°œë³„ ê¸°ì‚¬ ì¶”ì¶œ
                    stocks = news_data.get('stocks', {})
                    all_articles = []
                    
                    for stock_name, stock_data in stocks.items():
                        articles = stock_data.get('items', [])
                        for article in articles:
                            article['stock_name'] = stock_name
                        all_articles.extend(articles)
                    
                    print(f"      ğŸ“° ì´ {len(all_articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ë°œê²¬")
                    
                    # ê° ê¸°ì‚¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
                    for article_index, article in enumerate(all_articles):
                        print(f"        ğŸ“° ê¸°ì‚¬ {article_index+1} ì²˜ë¦¬ ì‹œì‘")
                        
                        # ê°œë³„ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        article_text = vector_manager._single_article_to_text(article, article_index)
                        print(f"          ğŸ“ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(article_text)}ì")
                        
                        # ì²­í‚¹
                        chunks = vector_manager._segment_text_with_clova(article_text, max_length=1024)
                        print(f"          ğŸ”„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                        
                        # ë©”íƒ€ë°ì´í„° ìƒì„±
                        for chunk_index, chunk in enumerate(chunks):
                            metadata_entry = {
                                "filename": json_file.name,
                                "type": "news",
                                "article_index": article_index,
                                "chunk_index": chunk_index,
                                "total_articles": len(all_articles),
                                "total_chunks": len(chunks),
                                "title": article.get('title', ''),
                                "text_content": chunk,
                                "text_length": len(chunk),
                                "created_at": "2025-01-31T00:00:00"
                            }
                            new_metadata.append(metadata_entry)
                            print(f"          ğŸ“ ì²­í¬ {chunk_index+1}: {len(chunk)}ì")
                            
                except Exception as e:
                    print(f"    âŒ JSON íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°ë¥¼ ë²¡í„° ë§¤ë‹ˆì €ì— ì €ì¥
            vector_manager.metadata = new_metadata
            
            # ë²¡í„° ìƒì„±
            print(f"\nğŸ” ë²¡í„° ìƒì„± ì‹œì‘...")
            new_vectors = []
            
            for i, metadata_entry in enumerate(new_metadata):
                try:
                    text_content = metadata_entry['text_content']
                    print(f"  ğŸ“ ë²¡í„° ìƒì„± ì¤‘: {i+1}/{len(new_metadata)} - {len(text_content)}ì")
                    
                    # í…ìŠ¤íŠ¸ ì„ë² ë”©
                    embedding = vector_manager.embedding_client.get_embedding(text_content)
                    if embedding:
                        new_vectors.append(embedding)
                        print(f"    âœ… ë²¡í„° ìƒì„± ì„±ê³µ")
                    else:
                        print(f"    âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"    âŒ ë²¡í„° ìƒì„± ì˜¤ë¥˜: {e}")
            
            vector_manager.vectors = new_vectors
            success = len(new_metadata) > 0 and len(new_vectors) > 0
            
            if success:
                print("âœ… data_1 í´ë” íŒŒì¼ë“¤ ì„ë² ë”© ì™„ë£Œ")
                print(f"ğŸ“Š ìƒì„±ëœ ë²¡í„°: {len(vector_manager.vectors)}ê°œ")
                print(f"ğŸ“Š ìƒì„±ëœ ë©”íƒ€ë°ì´í„°: {len(vector_manager.metadata)}ê°œ")
                
                # ë²¡í„° íŒŒì¼ ì €ì¥
                vector_file = vector_db_1_path / "hybrid_vectors.pkl"
                metadata_file = vector_db_1_path / "hybrid_metadata.json"
                
                import pickle
                with open(vector_file, 'wb') as f:
                    pickle.dump(vector_manager.vectors, f)
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(vector_manager.metadata, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ’¾ ë²¡í„° íŒŒì¼ ì €ì¥ ì™„ë£Œ: {vector_file}")
                print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ ì™„ë£Œ: {metadata_file}")
            else:
                print("âŒ data_1 í´ë” íŒŒì¼ë“¤ ì„ë² ë”© ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ data_1 í´ë” ì„ë² ë”© ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("\n" + "=" * 60)
        print("ğŸ” data_1 í´ë” ì„ë² ë”© ê±´ë„ˆë›°ê¸°")
        print("=" * 60)
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ OFF - data_1 í´ë” ì„ë² ë”© ê±´ë„ˆë›°ê¸°")
    
    # 8. Vector DB ê¸°ë°˜ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
    print("\n" + "=" * 60)
    print("ğŸ“Š Vector DB ê¸°ë°˜ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ")
    print("=" * 60)
    
    try:
        # Vector DB ë¶„ì„ê¸° í´ë˜ìŠ¤ ì •ì˜
        class VectorDBAnalyzer:
            """vector_db ë²¡í„° ê¸°ë°˜ ë¶„ì„ê¸° (FAISS API ì‚¬ìš©)"""
            
            def __init__(self):
                self.api_base_url = "http://localhost:8000"  # ë©”ì¸ FAISS API ì„œë²„
                
                # ìƒˆë¡œìš´ CLOVA API ì„¤ì • (vector_db_1_analyzerì™€ ë™ì¼)
                self.api_key = os.getenv("NEW_CLOVA_API_KEY", "")
                self.request_id = os.getenv("NEW_CLOVA_REQUEST_ID", "4997d0ab4e434139bd982084de885077")
                self.model_endpoint = os.getenv("NEW_CLOVA_MODEL_ENDPOINT", "/v3/tasks/yl1fvofj/chat-completions")
                
                # CLOVA í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                self.clova_client = self._create_clova_client()
                
                print("ğŸ”§ VectorDBAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
                print(f"ğŸ“¡ FAISS API ì„œë²„: {self.api_base_url}")
            
            def _create_clova_client(self):
                """CLOVA í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
                class CompletionExecutor:
                    def __init__(self, host, api_key, request_id, model_endpoint=None):
                        self._host = host
                        self._api_key = api_key
                        self._request_id = request_id
                        self._model_endpoint = model_endpoint or "/v3/tasks/yl1fvofj/chat-completions"

                    def _send_request(self, completion_request):
                        headers = {
                            'Content-Type': 'application/json; charset=utf-8',
                            'Authorization': self._api_key,
                            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
                        }

                        print(f"ğŸ” CLOVA API ìš”ì²­ ì •ë³´:")
                        print(f"   í˜¸ìŠ¤íŠ¸: {self._host}")
                        print(f"   ì—”ë“œí¬ì¸íŠ¸: {self._model_endpoint}")
                        print(f"   Request ID: {self._request_id}")
                        print(f"   API í‚¤: {self._api_key[:20]}...")

                        import http.client
                        conn = http.client.HTTPSConnection(self._host)
                        conn.request('POST', self._model_endpoint, json.dumps(completion_request), headers)
                        response = conn.getresponse()
                        
                        print(f"ğŸ“¡ CLOVA API ì‘ë‹µ:")
                        print(f"   ìƒíƒœ ì½”ë“œ: {response.status}")
                        
                        result = json.loads(response.read().decode(encoding='utf-8'))
                        conn.close()
                        return result

                    def execute(self, completion_request):
                        res = self._send_request(completion_request)
                        if res['status']['code'] == '20000':
                            # ìƒˆë¡œìš´ ëª¨ë¸ì€ message.content í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
                            if 'result' in res and 'message' in res['result'] and 'content' in res['result']['message']:
                                return res['result']['message']['content']
                            # ê¸°ì¡´ text í˜•ì‹ë„ ì§€ì›
                            elif 'result' in res and 'text' in res['result']:
                                return res['result']['text']
                            else:
                                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {res}")
                                return 'Error'
                        else:
                            return 'Error'
                
                return CompletionExecutor(
                    host='clovastudio.stream.ntruss.com',
                    api_key=f'Bearer {self.api_key}',
                    request_id=self.request_id,
                    model_endpoint=self.model_endpoint
                )
            
            def check_faiss_api_server(self) -> bool:
                """FAISS API ì„œë²„ ìƒíƒœ í™•ì¸"""
                try:
                    response = requests.get(f"{self.api_base_url}/health", timeout=5)
                    if response.status_code == 200:
                        data = response.json()
                        print(f"âœ… FAISS API ì„œë²„ ì—°ê²° ì„±ê³µ")
                        print(f"   ë²¡í„° ë¡œë“œ: {data.get('total_vectors', 0)}ê°œ")
                        print(f"   ë©”íƒ€ë°ì´í„°: {data.get('total_metadata', 0)}ê°œ")
                        return True
                    else:
                        print(f"âŒ FAISS API ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                        return False
                except Exception as e:
                    print(f"âŒ FAISS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
                    return False
            
            def search_vectors(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
                """FAISS APIë¥¼ í†µí•œ ë²¡í„° ê²€ìƒ‰"""
                try:
                    response = requests.post(
                        f"{self.api_base_url}/search",
                        json={"query": query, "top_k": top_k},
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        return data.get('results', [])
                    else:
                        print(f"âŒ ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                        return []
                        
                except requests.exceptions.Timeout:
                    print(f"âŒ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ (60ì´ˆ)")
                    return []
                except Exception as e:
                    print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    return []
            
            def analyze_vectors(self) -> str:
                """ë²¡í„° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±"""
                print("ğŸ” vector_db ë²¡í„° ë°ì´í„° ë¶„ì„ ì¤‘...")
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •
                search_query = "ì–´ì œ í•˜ë£¨ ë‰´ìŠ¤ ë° ì¼ì¼ ê±°ë˜ë°ì´í„° ì´ìŠˆ ìš”ì•½"
                
                # FAISS ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
                search_results = self.search_vectors(search_query, top_k=10)
                
                if not search_results:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return ""
                
                print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                text_contents = []
                for result in search_results:
                    text_content = result.get('text_content', '')
                    if text_content:
                        text_contents.append(text_content)
                
                # ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                analysis_prompt = self._create_analysis_prompt(text_contents)
                
                print("ğŸ¤– CLOVA ëª¨ë¸ì—ê²Œ ë³´ê³ ì„œ ìš”ì²­ ì¤‘...")
                
                # ìƒˆë¡œìš´ CLOVA ëª¨ë¸ í˜•ì‹ ì‚¬ìš© (messages í˜•ì‹)
                request_data = {
                    "messages": [
                        {
                            "role": "user",
                            "content": analysis_prompt
                        }
                    ],
                    "maxTokens": 4000,  # 4000ì ë³´ê³ ì„œ
                    "temperature": 0.7,
                    "topP": 0.8
                }
                
                # CLOVA API í˜¸ì¶œ
                response_text = self.clova_client.execute(request_data)
                
                if response_text and response_text != 'Error':
                    print(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {len(response_text)}ì")
                    return response_text
                else:
                    print("âŒ CLOVA API í˜¸ì¶œ ì‹¤íŒ¨")
                    return ""
                
            def _create_analysis_prompt(self, text_contents: List[str]) -> str:
                """ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
                prompt = f"""
ë‹¹ì‹ ì€ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ì œ í•˜ë£¨ì˜ ë‰´ìŠ¤ ë° ì¼ì¼ ê±°ë˜ë°ì´í„°ë¥¼ ìš”ì•½í•œ ë³´ê³ ì„œë¥¼ ìµœëŒ€í•œ ìì„¸íˆ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒì€ ë¶„ì„í•  ë°ì´í„°ì…ë‹ˆë‹¤:

{chr(15).join(text_contents[:15])}  # ì²˜ìŒ 10ê°œ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©

ë³´ê³ ì„œ ì‘ì„± ìš”êµ¬ì‚¬í•­:
1. ì´ ê¸€ììˆ˜: 4000ì ì •ë„
2. êµ¬ì¡°:
   - ì„œë¡ : ì£¼ìš” ì¦ì‹œ ìš”ì•½
   - ë³¸ë¡ : ë‰´ìŠ¤ì— ê¸°ë°˜í•œ ì–´ì œ í•˜ë£¨ì˜ ì£¼ìš” ì´ìŠˆ ìš”ì•½
   - ê²°ë¡ : ì–´ì œ í•˜ë£¨ì˜ ì¢…í•©ì ì¸ ì£¼ì‹ ì‹œì¥ ë¶„ì„

3. ê° ì¢…ëª©ë³„ ë¶„ì„ ë‚´ìš©:
   - ì–´ì œ ê±°ë˜ ë™í–¥ (ì¢…ëª©ëª…, ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰)
   - ê´€ë ¨ ë‰´ìŠ¤ ë° ì´ìŠˆ


4. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±
5. êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ê·¼ê±° ì œì‹œ
6. ì–´ì œ í•˜ë£¨ì˜ ì‹œì¥ ì „ë°˜ì ì¸ ë¶„ìœ„ê¸° ë¶„ì„

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ì œ í•˜ë£¨ì˜ ì¢…í•©ì ì¸ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
                return prompt
            
            def save_report(self, report: str) -> str:
                """ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
                try:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    report_file = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_2") / f"vector_db_report_{timestamp}.txt"
                    
                    with open(report_file, 'w', encoding='utf-8') as f:
                        f.write(f"# Vector DB ê¸°ë°˜ ì–´ì œ í•˜ë£¨ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ\n")
                        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ë¶„ì„ ë²¡í„° ìˆ˜: {len(report)}ì\n")
                        f.write(f"=" * 60 + "\n\n")
                        f.write(report)
                    
                    print(f"ğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_file}")
                    return str(report_file)
                    
                except Exception as e:
                    print(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
                    return ""
            
            def run_analysis(self) -> bool:
                """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
                print("\n" + "=" * 60)
                print("ğŸ” Vector DB ê¸°ë°˜ ì–´ì œ í•˜ë£¨ ì£¼ì‹ ì‹œì¥ ë¶„ì„")
                print("=" * 60)
                
                # 1. FAISS API ì„œë²„ ìƒíƒœ í™•ì¸
                if not self.check_faiss_api_server():
                    return False
                
                # 2. ë¶„ì„ ì‹¤í–‰
                report = self.analyze_vectors()
                if not report:
                    return False
                
                # 3. ë³´ê³ ì„œ ì €ì¥
                report_file = self.save_report(report)
                if not report_file:
                    return False
                
                # 4. ê²°ê³¼ ì¶œë ¥
                print("\n" + "=" * 60)
                print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ì „ì²´ ë³´ê³ ì„œ")
                print("=" * 60)
                print(report)
                
                return True
        
        # Vector DB ë¶„ì„ ì‹¤í–‰
        analyzer = VectorDBAnalyzer()
        analysis_success = analyzer.run_analysis()
        
        if analysis_success:
            print("âœ… Vector DB ê¸°ë°˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        else:
            print("âŒ Vector DB ê¸°ë°˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ Vector DB ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # 9. Vector DB1 FAISS API ì„œë²„ ì‹œì‘
    if not start_vector_db1_api_server():
        print("âŒ Vector DB1 FAISS API ì„œë²„ ì‹œì‘ ì‹¤íŒ¨ë¡œ ì¸í•´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        # 10. vector_db_1 ê¸°ë°˜ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        print("\n" + "=" * 60)
        print("ğŸ“Š Vector DB1 ê¸°ë°˜ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ")
        print("=" * 60)
        
        try:
            from vector_db_1_analyzer import VectorDB1Analyzer
            analyzer = VectorDB1Analyzer()
            analysis_success = analyzer.run_analysis()
            
            if analysis_success:
                print("âœ… Vector DB1 ê¸°ë°˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            else:
                print("âŒ Vector DB1 ê¸°ë°˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ Vector DB1 ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # 11. ë‘ ë³´ê³ ì„œ í•©ì¹˜ê¸° ë° ì €ì¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ë‘ ë³´ê³ ì„œ í•©ì¹˜ê¸° ë° ì €ì¥")
    print("=" * 60)
    
    try:
        # daily_report í´ë” ìƒì„±
        daily_report_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/daily_report")
        daily_report_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ daily_report í´ë” ìƒì„±/í™•ì¸: {daily_report_dir}")
        
        # ìµœê·¼ ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ë“¤ ì°¾ê¸°
        data_2_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_2")
        
        # Vector DB ë³´ê³ ì„œ ì°¾ê¸°
        vector_db_reports = list(data_2_dir.glob("vector_db_report_*.txt"))
        vector_db_report = None
        if vector_db_reports:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
            vector_db_report = max(vector_db_reports, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“„ Vector DB ë³´ê³ ì„œ ë°œê²¬: {vector_db_report.name}")
        
        # Vector DB1 ë³´ê³ ì„œ ì°¾ê¸°
        vector_db1_reports = list(data_2_dir.glob("vector_db_1_report_*.txt"))
        vector_db1_report = None
        if vector_db1_reports:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
            vector_db1_report = max(vector_db1_reports, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“„ Vector DB1 ë³´ê³ ì„œ ë°œê²¬: {vector_db1_report.name}")
        
        # ë³´ê³ ì„œ ë‚´ìš© ì½ê¸°
        combined_content = []
        combined_content.append("# ì¼ì¼ ì£¼ì‹ ì‹œì¥ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
        combined_content.append(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        combined_content.append("=" * 80)
        combined_content.append("")
        
        # Vector DB ë³´ê³ ì„œ ì¶”ê°€
        if vector_db_report:
            try:
                with open(vector_db_report, 'r', encoding='utf-8') as f:
                    content = f.read()
                    combined_content.append("## ğŸ“Š ì–´ì œ í•˜ë£¨ ì£¼ì‹ ì‹œì¥ ë¶„ì„")
                    combined_content.append("")
                    combined_content.append(content)
                    combined_content.append("")
                    combined_content.append("=" * 80)
                    combined_content.append("")
                    print(f"âœ… Vector DB ë³´ê³ ì„œ ë‚´ìš© ì¶”ê°€: {len(content)}ì")
            except Exception as e:
                print(f"âŒ Vector DB ë³´ê³ ì„œ ì½ê¸° ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ Vector DB ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Vector DB1 ë³´ê³ ì„œ ì¶”ê°€
        if vector_db1_report:
            try:
                with open(vector_db1_report, 'r', encoding='utf-8') as f:
                    content = f.read()
                    combined_content.append("## ğŸ“ˆ ì˜¤ëŠ˜ ì´ìŠˆê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì£¼ëª© ì¢…ëª© ë¶„ì„")
                    combined_content.append("")
                    combined_content.append(content)
                    print(f"âœ… Vector DB1 ë³´ê³ ì„œ ë‚´ìš© ì¶”ê°€: {len(content)}ì")
            except Exception as e:
                print(f"âŒ Vector DB1 ë³´ê³ ì„œ ì½ê¸° ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ Vector DB1 ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•©ì³ì§„ ë³´ê³ ì„œ ì €ì¥
        if len(combined_content) > 4:  # ê¸°ë³¸ í—¤ë” ì™¸ì— ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            combined_report_file = daily_report_dir / f"daily_combined_report_{timestamp}.txt"
            
            with open(combined_report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(combined_content))
            
            combined_text = '\n'.join(combined_content)
            print(f"ğŸ’¾ í•©ì³ì§„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {combined_report_file}")
            print(f"ğŸ“Š ì´ ê¸¸ì´: {len(combined_text)}ì")
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“‹ ë³´ê³ ì„œ ìš”ì•½:")
            if vector_db_report:
                print(f"  ğŸ“Š Vector DB ë³´ê³ ì„œ: {vector_db_report.name}")
            if vector_db1_report:
                print(f"  ğŸ“ˆ Vector DB1 ë³´ê³ ì„œ: {vector_db1_report.name}")
            print(f"  ğŸ“„ í•©ì³ì§„ ë³´ê³ ì„œ: {combined_report_file.name}")
            
        else:
            print("âŒ í•©ì¹  ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ë³´ê³ ì„œ í•©ì¹˜ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\nğŸ‰ í†µí•© ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 