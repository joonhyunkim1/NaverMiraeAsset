#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ê´€ë¦¬ ì‹œìŠ¤í…œ
- LlamaIndexì˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìœ ì§€
- CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ API ì‚¬ìš©
- CLOVA ì„ë² ë”© APIë¡œ ì§ì ‘ ì €ì¥
"""

import json
import os
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime
import pandas as pd

from clova_embedding import ClovaEmbeddingAPI
from clova_segmentation import ClovaSegmentationClient
from news_content_extractor import NewsContentExtractor


class HybridVectorManager:
    """í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ê´€ë¦¬ ì‹œìŠ¤í…œ (LlamaIndex + CLOVA)"""
    
    def __init__(self, data_dir: str = "/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data"):
        self.data_dir = Path(data_dir)
        self.vector_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/vector_db")
        self.vector_dir.mkdir(exist_ok=True)
        
        # CLOVA í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.embedding_client = ClovaEmbeddingAPI()
        self.segmentation_client = ClovaSegmentationClient()
        self.news_extractor = NewsContentExtractor()
        
        # ë²¡í„° ì €ì¥ì†Œ
        self.vectors_file = self.vector_dir / "hybrid_vectors.pkl"
        self.metadata_file = self.vector_dir / "hybrid_metadata.json"
        
        # ë²¡í„° ë°ì´í„° ë¡œë“œ
        self.vectors = []
        self.metadata = []
        self._load_vectors()
    
    def _load_vectors(self):
        """ì €ì¥ëœ ë²¡í„° ë°ì´í„° ë¡œë“œ"""
        try:
            if self.vectors_file.exists():
                with open(self.vectors_file, 'rb') as f:
                    self.vectors = pickle.load(f)
                print(f"âœ… ê¸°ì¡´ ë²¡í„° ë¡œë“œ ì™„ë£Œ: {len(self.vectors)}ê°œ")
            
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                print(f"âœ… ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ")
                
        except Exception as e:
            print(f"âš ï¸ ë²¡í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.vectors = []
            self.metadata = []
    
    def _save_vectors(self):
        """ë²¡í„° ë°ì´í„° ì €ì¥"""
        try:
            with open(self.vectors_file, 'wb') as f:
                pickle.dump(self.vectors, f)
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ: {len(self.vectors)}ê°œ")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def process_documents(self, rebuild: bool = False) -> bool:
        """ë¬¸ì„œë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹ + CLOVA ì €ì¥)"""
        if rebuild:
            print("ğŸ”„ ë²¡í„° ì¬êµ¬ì¶• ëª¨ë“œ")
            self.vectors = []
            self.metadata = []
        
        print("ğŸ“š ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
        
        # CSV íŒŒì¼ ì²˜ë¦¬
        csv_success = self._process_csv_files()
        
        # ë‰´ìŠ¤ íŒŒì¼ ì²˜ë¦¬
        news_success = self._process_news_files()
        
        if csv_success or news_success:
            self._save_vectors()
            print(f"ğŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(self.vectors)}ê°œ ë²¡í„°")
            return True
        
        print("âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨")
        return False
    
    def _process_csv_files(self) -> bool:
        """CSV íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹)"""
        csv_files = list(self.data_dir.glob("*.csv"))
        
        if not csv_files:
            print("ğŸ“ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“Š CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘: {len(csv_files)}ê°œ")
        
        for csv_file in csv_files:
            try:
                print(f"  ğŸ“„ ì²˜ë¦¬ ì¤‘: {csv_file.name}")
                
                # CSV íŒŒì¼ ì½ê¸°
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                # DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)
                text_content = self._dataframe_to_text(df, csv_file.stem)
                
                # ì²« ë²ˆì§¸ í•­ëª© ë³€í™˜ ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê·¸)
                print(f"  ğŸ” ì²« ë²ˆì§¸ ì¢…ëª© ë³€í™˜ ê²°ê³¼:")
                lines = text_content.split('\n')
                for i, line in enumerate(lines[:10]):  # ì²˜ìŒ 10ì¤„ë§Œ ì¶œë ¥
                    if line.strip():
                        print(f"    {i+1:2d}: {line}")
                if len(lines) > 10:
                    print(f"    ... (ì´ {len(lines)}ì¤„)")
                
                print(f"  ğŸ”§ CSV ë°ì´í„° - ìµœì í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©")
                chunks = self._segment_text_with_clova(text_content, max_length=2048)
                
                # ê° ì²­í¬ë¥¼ CLOVA ì„ë² ë”© APIë¡œ ë²¡í„°í™” (ì§ì ‘ ì €ì¥)
                for i, chunk in enumerate(chunks):
                    vector = self.embedding_client.get_text_embedding(chunk)
                    
                    if vector:
                        self.vectors.append(vector)
                        self.metadata.append({
                            "filename": csv_file.name,
                            "type": "csv",
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "rows": len(df),
                            "columns": list(df.columns),
                            "text_content": chunk,  # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ê°€
                            "text_length": len(chunk),
                            "created_at": datetime.now().isoformat()
                        })
                        print(f"    âœ… ì²­í¬ {i+1}/{len(chunks)} ë²¡í„°í™” ì™„ë£Œ")
                    else:
                        print(f"    âŒ ì²­í¬ {i+1} ë²¡í„°í™” ì‹¤íŒ¨")
                
            except Exception as e:
                print(f"  âŒ {csv_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return True
    
    def _process_news_files(self) -> bool:
        """ë‰´ìŠ¤ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹)"""
        news_files = list(self.data_dir.glob("*news*.json"))
        
        if not news_files:
            print("ğŸ“° ë‰´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“° ë‰´ìŠ¤ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {len(news_files)}ê°œ")
        
        for news_file in news_files:
            try:
                print(f"  ğŸ“„ ì²˜ë¦¬ ì¤‘: {news_file.name}")
                
                # JSON íŒŒì¼ ì½ê¸°
                with open(news_file, 'r', encoding='utf-8') as f:
                    news_data = json.load(f)
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)
                articles = news_data.get('items', [])
                
                for i, article in enumerate(articles):
                    # ë³¸ë¬¸ ì „ì²´ ì¶”ì¶œ ì‹œë„
                    full_content = self._get_full_article_content(article)
                    
                    if full_content:
                        # ë³¸ë¬¸ ì „ì²´ê°€ ìˆëŠ” ê²½ìš°
                        text_content = self._article_to_text_with_full_content(article, i, full_content)
                    else:
                        # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                        text_content = self._article_to_text(article, i)
                    
                    # ì²« ë²ˆì§¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë³€í™˜ ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê·¸)
                    if i == 0:
                        print(f"  ğŸ” ì²« ë²ˆì§¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë³€í™˜ ê²°ê³¼:")
                        lines = text_content.split('\n')
                        for j, line in enumerate(lines):
                            if line.strip():
                                print(f"    {j+1:2d}: {line}")
                    
                    # CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ APIë¡œ ì²­í‚¹ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)
                    print(f"  ğŸ“° ë‰´ìŠ¤ ë°ì´í„° - ê¸°ë³¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©")
                    chunks = self._segment_text_with_clova(text_content, max_length=512)
                    
                    # ê° ì²­í¬ë¥¼ CLOVA ì„ë² ë”© APIë¡œ ë²¡í„°í™” (ì§ì ‘ ì €ì¥)
                    for j, chunk in enumerate(chunks):
                        vector = self.embedding_client.get_text_embedding(chunk)
                        
                        if vector:
                            self.vectors.append(vector)
                            self.metadata.append({
                                "filename": news_file.name,
                                "type": "news",
                                "article_index": i,
                                "chunk_index": j,
                                "total_articles": len(articles),
                                "total_chunks": len(chunks),
                                "title": article.get('title', ''),
                                "text_content": chunk,  # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ê°€
                                "text_length": len(chunk),
                                "created_at": datetime.now().isoformat()
                            })
                            print(f"    âœ… ê¸°ì‚¬ {i+1} ì²­í¬ {j+1}/{len(chunks)} ë²¡í„°í™” ì™„ë£Œ")
                        else:
                            print(f"    âŒ ê¸°ì‚¬ {i+1} ì²­í¬ {j+1} ë²¡í„°í™” ì‹¤íŒ¨")
                
            except Exception as e:
                print(f"  âŒ {news_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return True
    
    def _dataframe_to_text(self, df: pd.DataFrame, filename: str) -> str:
        """DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)"""
        text_parts = []
        
        # íŒŒì¼ëª… ì •ë³´
        text_parts.append(f"íŒŒì¼ëª…: {filename}")
        text_parts.append(f"ì´ {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        text_parts.append("")
        
        # KRX ë°ì´í„°ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if 'ISU_ABBRV' in df.columns and 'TDD_CLSPRC' in df.columns:
            text_parts.append("=== KRX ì¼ì¼ê±°ë˜ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§) ===")
            text_parts.append("")
            
            # ì „ì²´ ë°ì´í„°ì—ì„œ ê±°ë˜ëŒ€ê¸ˆ í•˜ìœ„ 70%ì™€ ë“±ë½ë¥  í•˜ìœ„ 70%ì—ì„œ ê²¹ì¹˜ëŠ” ì£¼ì‹ ì œì™¸
            df_processed = df.copy()
            
            # FLUC_RT ì»¬ëŸ¼ì„ ë“±ë½ë¥ ë¡œ ì‚¬ìš©
            df_processed['ë“±ë½ë¥ '] = df_processed['FLUC_RT']
            df_processed['ë“±ë½ë¥ _ì ˆëŒ€ê°’'] = df_processed['ë“±ë½ë¥ '].abs()
            
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df_processed)}ê°œ ì¢…ëª©")
            
            # ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸°
            df_trading_value_sorted = df_processed.sort_values('ACC_TRDVAL', ascending=True)
            bottom_70_percent_trading = int(len(df_trading_value_sorted) * 0.7)
            low_trading_stocks = set(df_trading_value_sorted.head(bottom_70_percent_trading)['ISU_ABBRV'].tolist())
            
            # ë“±ë½ë¥  ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸° (ì ˆëŒ€ê°’ ê¸°ì¤€)
            df_change_rate_sorted = df_processed.sort_values('ë“±ë½ë¥ _ì ˆëŒ€ê°’', ascending=True)
            bottom_70_percent_change = int(len(df_change_rate_sorted) * 0.7)
            low_change_stocks = set(df_change_rate_sorted.head(bottom_70_percent_change)['ISU_ABBRV'].tolist())
            
            # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ ì°¾ê¸°
            overlapping_stocks = low_trading_stocks.intersection(low_change_stocks)
            
            print(f"ğŸ“Š ê±°ë˜ëŒ€ê¸ˆ í•˜ìœ„ 70%: {len(low_trading_stocks)}ê°œ ì¢…ëª©")
            print(f"ğŸ“Š ë“±ë½ë¥  í•˜ìœ„ 70%: {len(low_change_stocks)}ê°œ ì¢…ëª©")
            print(f"ğŸ“Š ê²¹ì¹˜ëŠ” ì¢…ëª©: {len(overlapping_stocks)}ê°œ")
            
            # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ì„ ì œì™¸í•œ ìµœì¢… í•„í„°ë§
            df_final_filtered = df_processed[~df_processed['ISU_ABBRV'].isin(overlapping_stocks)]
            
            print(f"ğŸ“Š ìµœì¢… í•„í„°ë§ ê²°ê³¼: {len(df_processed)}ê°œ â†’ {len(df_final_filtered)}ê°œ")
            
            # ê° ì¢…ëª©ë³„ë¡œ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
            for idx, row in df_final_filtered.iterrows():
                stock_name = row.get('ISU_ABBRV', '')  # ì‹¤ì œ ì»¬ëŸ¼ëª…
                stock_code = row.get('ISU_CD', '')     # ì‹¤ì œ ì»¬ëŸ¼ëª…
                
                if stock_name and stock_code:
                    # ê°€ê²© ì •ë³´
                    open_price = row.get('TDD_OPNPRC', 0)
                    high_price = row.get('TDD_HGPRC', 0)
                    low_price = row.get('TDD_LWPRC', 0)
                    close_price = row.get('TDD_CLSPRC', 0)
                    trading_value = row.get('ACC_TRDVAL', 0)
                    change_rate = row.get('ë“±ë½ë¥ ', 0.0)
                    
                    # ì¢…ëª© ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ êµ¬ì„±
                    stock_info = f"ì¢…ëª©ëª…: {stock_name}, ì‹œê°€: {open_price}, ê³ ê°€: {high_price}, ì €ê°€: {low_price}, ì¢…ê°€: {close_price}, ê±°ë˜ëŒ€ê¸ˆ: {trading_value}, ë“±ë½ë¥ : {change_rate:.2f}"
                    text_parts.append(stock_info)
            
            text_parts.append("")
            text_parts.append(f"ì´ {len(df_final_filtered)}ê°œ ì¢…ëª©ì˜ ê±°ë˜ ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§)")
        else:
            # ì¼ë°˜ CSV íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            text_parts.append("ì»¬ëŸ¼ëª…:")
            for col in df.columns:
                text_parts.append(f"  - {col}")
            text_parts.append("")
            
            # ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰)
            text_parts.append("ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰):")
            sample_df = df.head(10)
            text_parts.append(sample_df.to_string(index=False))
        
        return "\n".join(text_parts)
    
    def _article_to_text(self, article: Dict, index: int) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)"""
        text_parts = []
        
        # ê¸°ì‚¬ ì œëª©
        title = article.get('title', '')
        if title:
            text_parts.append(f"ì œëª©: {title}")
        
        # ê¸°ì‚¬ ë‚´ìš© (ì„¤ëª…)
        description = article.get('description', '')
        if description:
            # HTML íƒœê·¸ ì œê±°
            import re
            clean_description = re.sub(r'<[^>]+>', '', description)
            text_parts.append(f"ë‚´ìš©: {clean_description}")
        
        # ë°œí–‰ì¼
        pub_date = article.get('pubDate', '')
        if pub_date:
            text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
        
        return "\n".join(text_parts)
    
    def _segment_text_with_clova(self, text: str, max_length: int = 512) -> List[str]:
        """CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ APIë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)"""
        try:
            # CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ API ì‚¬ìš© (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)
            segments = self.segmentation_client.segment_text(text, max_length)
            if segments:
                print(f"    ğŸ“ CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ: {len(segments)}ê°œ ì²­í¬")
                return segments
        except Exception as e:
            print(f"âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ API ì˜¤ë¥˜: {e}")
        
        # í´ë°±: ê°„ë‹¨í•œ ì²­í‚¹ (LlamaIndex ë°©ì‹ê³¼ ë™ì¼)
        print(f"    ğŸ“ í´ë°± ì²­í‚¹ ì‚¬ìš©: {max_length}ì ë‹¨ìœ„")
        chunks = []
        words = text.split()
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word) + 1  # ê³µë°± í¬í•¨
            if current_length + word_length > max_length and current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = [word]
                current_length = word_length
            else:
                current_chunk.append(word)
                current_length += word_length
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def get_vectors_for_analysis(self) -> tuple[List[List[float]], List[Dict[str, Any]]]:
        """ë¶„ì„ìš© ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        return self.vectors, self.metadata
    
    def get_vector_info(self) -> Dict[str, Any]:
        """ë²¡í„° ì •ë³´ ë°˜í™˜"""
        return {
            "total_vectors": len(self.vectors),
            "vector_dimension": len(self.vectors[0]) if self.vectors else 0,
            "metadata_count": len(self.metadata),
            "vector_file": str(self.vectors_file),
            "metadata_file": str(self.metadata_file)
        }

    def _get_full_article_content(self, article: Dict) -> Optional[str]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì „ì²´ë¥¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì›ë³¸ ë§í¬ ìš°ì„ , ì—†ìœ¼ë©´ ë„¤ì´ë²„ ë§í¬ ì‚¬ìš©
            url = article.get('originallink') or article.get('link')
            if not url:
                return None
            
            # NewsContentExtractorë¥¼ ì‚¬ìš©í•´ì„œ ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self.news_extractor.extract_article_content(url)
            if content_data and content_data.get('content'):
                return content_data['content']
            
            return None
            
        except Exception as e:
            print(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _article_to_text_with_full_content(self, article: Dict, index: int, full_content: str) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë³¸ë¬¸ ì „ì²´ í¬í•¨)"""
        text_parts = []
        
        # ê¸°ì‚¬ ì œëª©
        title = article.get('title', '')
        if title:
            text_parts.append(f"ì œëª©: {title}")
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ì „ì²´
        text_parts.append(f"ë³¸ë¬¸: {full_content}")
        
        # ë°œí–‰ì¼
        pub_date = article.get('pubDate', '')
        if pub_date:
            text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
        
        return "\n".join(text_parts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    manager = HybridVectorManager()
    success = manager.process_documents(rebuild=True)
    
    if success:
        info = manager.get_vector_info()
        print(f"âœ… ë²¡í„° ì²˜ë¦¬ ì™„ë£Œ: {info}")
    else:
        print("âŒ ë²¡í„° ì²˜ë¦¬ ì‹¤íŒ¨") 