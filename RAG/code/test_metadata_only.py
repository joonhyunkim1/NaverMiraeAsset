#!/usr/bin/env python3
"""
ë©”íƒ€ë°ì´í„°ë§Œ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
- ê¸°ì¡´ ë²¡í„° íŒŒì¼ì€ ìœ ì§€
- ë©”íƒ€ë°ì´í„°ë§Œ ìƒˆë¡œ ìƒì„±
- ë‰´ìŠ¤ ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
"""

import json
import pickle
from pathlib import Path
from hybrid_vector_manager import HybridVectorManager
from clova_embedding import ClovaEmbeddingAPI
from clova_segmentation import ClovaSegmentationClient
from news_content_extractor import NewsContentExtractor

def test_metadata_generation():
    """ë©”íƒ€ë°ì´í„°ë§Œ ìƒˆë¡œ ìƒì„±í•˜ì—¬ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ” ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # data_1 í´ë”ìš© ë²¡í„° ë§¤ë‹ˆì € ìƒì„± (ë²¡í„° ì €ì¥ ì—†ì´)
    class MetadataOnlyVectorManager(HybridVectorManager):
        def __init__(self, data_dir: str):
            self.data_dir = Path(data_dir)
            self.vector_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/vector_db_1")
            
            # CLOVA í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.embedding_client = ClovaEmbeddingAPI()
            self.segmentation_client = ClovaSegmentationClient()
            self.news_extractor = NewsContentExtractor()
            
            # ë²¡í„° ì €ì¥ì†Œ (ì½ê¸° ì „ìš©)
            self.vectors_file = self.vector_dir / "hybrid_vectors.pkl"
            self.metadata_file = self.vector_dir / "hybrid_metadata.json"
            
            # ê¸°ì¡´ ë²¡í„° ë°ì´í„° ë¡œë“œ
            self.vectors = []
            self.metadata = []
            self._load_vectors()
        
        def process_documents(self):
            """ë¬¸ì„œ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„±)"""
            print("ğŸ“Š ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ (ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„±)...")
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
            new_metadata = []
            
            # CSV íŒŒì¼ ì²˜ë¦¬
            csv_files = list(self.data_dir.glob("*.csv"))
            print(f"ğŸ“„ CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
            
            for csv_file in csv_files:
                print(f"ğŸ” CSV íŒŒì¼ ì²˜ë¦¬: {csv_file.name}")
                try:
                    import pandas as pd
                    df = pd.read_csv(csv_file)
                    
                    # CSVë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    text_content = self._dataframe_to_text(df, csv_file.name)
                    
                    # ì²­í‚¹
                    chunks = self._segment_text_with_clova(text_content, max_length=2048)
                    
                    print(f"  ğŸ“Š ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„± (ë²¡í„°ëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ)
                    for i, chunk in enumerate(chunks):
                        metadata_entry = {
                            "filename": csv_file.name,
                            "type": "csv",
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "text_content": chunk,
                            "text_length": len(chunk),
                            "created_at": "2025-01-31T00:00:00"
                        }
                        new_metadata.append(metadata_entry)
                        print(f"    ğŸ“ ì²­í¬ {i+1}: {len(chunk)}ì")
                        
                except Exception as e:
                    print(f"  âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # JSON íŒŒì¼ ì²˜ë¦¬ (ë‰´ìŠ¤ ë°ì´í„°) - vector_db í˜•ì‹ìœ¼ë¡œ ê°œë³„ ê¸°ì‚¬ë³„ ì²˜ë¦¬
            json_files = list(self.data_dir.glob("*.json"))
            print(f"ğŸ“„ JSON íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
            
            for json_file in json_files:
                print(f"ğŸ” JSON íŒŒì¼ ì²˜ë¦¬: {json_file.name}")
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        news_data = json.load(f)
                    
                    # stocks êµ¬ì¡°ì—ì„œ ê°œë³„ ê¸°ì‚¬ ì¶”ì¶œ
                    stocks = news_data.get('stocks', {})
                    all_articles = []
                    
                    for stock_name, stock_data in stocks.items():
                        articles = stock_data.get('items', [])
                        for article in articles:
                            article['stock_name'] = stock_name
                        all_articles.extend(articles)
                    
                    print(f"  ğŸ“° ì´ {len(all_articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ë°œê²¬")
                    
                    # ê° ê¸°ì‚¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ (vector_db ë°©ì‹)
                    for article_index, article in enumerate(all_articles):
                        print(f"    ğŸ“° ê¸°ì‚¬ {article_index+1} ì²˜ë¦¬ ì‹œì‘")
                        
                        # ê°œë³„ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        article_text = self._single_article_to_text(article, article_index)
                        print(f"      ğŸ“ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(article_text)}ì")
                        print(f"      ğŸ“ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {article_text[:200]}...")
                        
                        # ì²­í‚¹ (ë” í° í¬ê¸°ë¡œ ì„¤ì •í•˜ì—¬ ì§¤ë¦¼ ë°©ì§€)
                        chunks = self._segment_text_with_clova(article_text, max_length=1024)
                        print(f"      ğŸ”„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                        
                        # ê° ì²­í¬ì˜ ê¸¸ì´ í™•ì¸
                        for chunk_idx, chunk in enumerate(chunks):
                            print(f"        ğŸ“„ ì²­í¬ {chunk_idx+1} ê¸¸ì´: {len(chunk)}ì")
                            print(f"        ğŸ“„ ì²­í¬ {chunk_idx+1} ë¯¸ë¦¬ë³´ê¸°: {chunk[:100]}...")
                        
                        print(f"    ğŸ“° ê¸°ì‚¬ {article_index+1} ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                        
                        # ê° ì²­í¬ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ìƒì„±
                        for chunk_index, chunk in enumerate(chunks):
                            metadata_entry = {
                                "filename": json_file.name,
                                "type": "news",
                                "article_index": article_index,
                                "chunk_index": chunk_index,
                                "total_articles": len(all_articles),
                                "total_chunks": len(chunks),
                                "title": article.get('title', ''),
                                "text_content": chunk,
                                "text_length": len(chunk),
                                "created_at": "2025-01-31T00:00:00"
                            }
                            new_metadata.append(metadata_entry)
                            print(f"      ğŸ“ ì²­í¬ {chunk_index+1}: {len(chunk)}ì")
                            print(f"      ğŸ“ ë©”íƒ€ë°ì´í„° text_content ê¸¸ì´: {len(metadata_entry['text_content'])}ì")
                        
                except Exception as e:
                    print(f"  âŒ JSON íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ:")
            print(f"  ğŸ“„ ì´ ë©”íƒ€ë°ì´í„° í•­ëª©: {len(new_metadata)}ê°œ")
            
            # íƒ€ì…ë³„ í†µê³„
            csv_count = len([m for m in new_metadata if m["type"] == "csv"])
            news_count = len([m for m in new_metadata if m["type"] == "news"])
            
            print(f"  ğŸ“Š CSV ë©”íƒ€ë°ì´í„°: {csv_count}ê°œ")
            print(f"  ğŸ“° ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„°: {news_count}ê°œ")
            
            # ìƒ˜í”Œ ì¶œë ¥
            if new_metadata:
                print(f"\nğŸ“ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ìƒì„¸ ë¶„ì„:")
                print("=" * 60)
                
                # CSV ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ
                csv_samples = [m for m in new_metadata if m["type"] == "csv"]
                if csv_samples:
                    print(f"\nğŸ“Š CSV ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸):")
                    csv_sample = csv_samples[0]
                    for key, value in csv_sample.items():
                        if key == "text_content":
                            print(f"  {key}: {str(value)[:100]}...")
                        else:
                            print(f"  {key}: {value}")
                
                # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ
                news_samples = [m for m in new_metadata if m["type"] == "news"]
                if news_samples:
                    print(f"\nğŸ“° ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸):")
                    news_sample = news_samples[0]
                    for key, value in news_sample.items():
                        if key == "text_content":
                            print(f"  {key}: {str(value)[:100]}...")
                        else:
                            print(f"  {key}: {value}")
                
                # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ (ë‘ ë²ˆì§¸)
                if len(news_samples) > 1:
                    print(f"\nğŸ“° ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ (ë‘ ë²ˆì§¸):")
                    news_sample2 = news_samples[1]
                    for key, value in news_sample2.items():
                        if key == "text_content":
                            print(f"  {key}: {str(value)[:100]}...")
                        else:
                            print(f"  {key}: {value}")
                
                print(f"\nğŸ“Š ì „ì²´ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ìš”ì•½:")
                print(f"  - ì´ í•­ëª© ìˆ˜: {len(new_metadata)}ê°œ")
                print(f"  - CSV í•­ëª©: {len(csv_samples)}ê°œ")
                print(f"  - ë‰´ìŠ¤ í•­ëª©: {len(news_samples)}ê°œ")
                
                # ê° íƒ€ì…ë³„ í•„ë“œ ë¶„ì„
                if csv_samples:
                    csv_fields = list(csv_samples[0].keys())
                    print(f"  - CSV í•„ë“œ: {csv_fields}")
                
                if news_samples:
                    news_fields = list(news_samples[0].keys())
                    print(f"  - ë‰´ìŠ¤ í•„ë“œ: {news_fields}")
            
            return len(new_metadata) > 0

        def _article_to_text(self, news_data: dict, filename: str) -> str:
            """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (vector_dbì™€ ë™ì¼í•œ ë°©ì‹)"""
            text_parts = []
            
            # vector_dbì™€ ë™ì¼í•œ ë°©ì‹: 'items' ë°°ì—´ ì‚¬ìš©
            articles = news_data.get('items', [])
            
            # ë§Œì•½ 'stocks' êµ¬ì¡°ë¼ë©´ 'items'ë¡œ ë³€í™˜
            if 'stocks' in news_data and not articles:
                all_articles = []
                stocks = news_data.get('stocks', {})
                for stock_name, stock_data in stocks.items():
                    stock_articles = stock_data.get('items', [])
                    # ê° ê¸°ì‚¬ì— ì¢…ëª©ëª… ì¶”ê°€
                    for article in stock_articles:
                        article['stock_name'] = stock_name
                    all_articles.extend(stock_articles)
                articles = all_articles
            
            text_parts.append(f"íŒŒì¼ëª…: {filename}")
            text_parts.append(f"ì´ {len(articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬")
            text_parts.append("")
            
            for i, article in enumerate(articles):
                text_parts.append(f"=== ë‰´ìŠ¤ {i+1} ===")
                
                # ì¢…ëª©ëª… (ìˆëŠ” ê²½ìš°)
                stock_name = article.get('stock_name', '')
                if stock_name:
                    text_parts.append(f"ì¢…ëª©: {stock_name}")
                
                # ê¸°ì‚¬ ì œëª©
                title = article.get('title', '')
                if title:
                    # HTML íƒœê·¸ ì œê±°
                    import re
                    clean_title = re.sub(r'<[^>]+>', '', title)
                    text_parts.append(f"ì œëª©: {clean_title}")
                
                # ê¸°ì‚¬ ë‚´ìš© (ì„¤ëª…)
                description = article.get('description', '')
                if description:
                    # HTML íƒœê·¸ ì œê±°
                    import re
                    clean_description = re.sub(r'<[^>]+>', '', description)
                    text_parts.append(f"ë‚´ìš©: {clean_description}")
                
                # ë°œí–‰ì¼
                pub_date = article.get('pubDate', '')
                if pub_date:
                    text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
                
                text_parts.append("")
            
            return "\n".join(text_parts)
        
        def _single_article_to_text(self, article: dict, article_index: int) -> str:
            """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (vector_db ë°©ì‹) - URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ"""
            text_parts = []
            
            # ê¸°ì‚¬ ì œëª©
            title = article.get('title', '')
            if title:
                # HTML íƒœê·¸ ì œê±°
                import re
                clean_title = re.sub(r'<[^>]+>', '', title)
                text_parts.append(f"ì œëª©: {clean_title}")
                print(f"        ğŸ“° ì œëª© ê¸¸ì´: {len(clean_title)}ì")
            
            # URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            full_content = None
            url = article.get('originallink') or article.get('link')
            
            if url:
                try:
                    print(f"      ğŸ”— URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„: {url}")
                    full_content = self.news_extractor.extract_article_content(url)
                    if full_content and full_content.get('content'):
                        print(f"      âœ… ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(full_content['content'])}ì")
                        print(f"      ğŸ“„ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {full_content['content'][:100]}...")
                    else:
                        print(f"      âš ï¸ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨, ìš”ì•½ë¬¸ ì‚¬ìš©")
                except Exception as e:
                    print(f"      âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # ì „ì²´ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìš”ì•½ë¬¸ ì‚¬ìš©
            if full_content and full_content.get('content'):
                content = full_content['content']
                # HTML íƒœê·¸ ì œê±°
                import re
                clean_content = re.sub(r'<[^>]+>', '', content)
                text_parts.append(f"ë³¸ë¬¸: {clean_content}")
                print(f"        ğŸ“„ ì •ì œëœ ë³¸ë¬¸ ê¸¸ì´: {len(clean_content)}ì")
                print(f"        ğŸ“„ ì •ì œëœ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {clean_content}...")
            else:
                # ìš”ì•½ë¬¸ ì‚¬ìš©
                description = article.get('description', '')
                if description:
                    # HTML íƒœê·¸ ì œê±°
                    import re
                    clean_description = re.sub(r'<[^>]+>', '', description)
                    text_parts.append(f"ë‚´ìš©: {clean_description}")
                    print(f"        ğŸ“„ ìš”ì•½ë¬¸ ê¸¸ì´: {len(clean_description)}ì")
                    print(f"        ğŸ“„ ìš”ì•½ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {clean_description[:100]}...")
            
            # ë°œí–‰ì¼
            pub_date = article.get('pubDate', '')
            if pub_date:
                text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
            
            final_text = "\n".join(text_parts)
            print(f"        ğŸ“ ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_text)}ì")
            print(f"        ğŸ“ ìµœì¢… í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {final_text}...")
            
            return final_text

        def _csv_to_summary_text(self, csv_file: Path) -> str:
            """CSV íŒŒì¼ì„ ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜"""
            try:
                import pandas as pd
                from datetime import datetime, timedelta
                
                # CSV íŒŒì¼ ì½ê¸°
                df = pd.read_csv(csv_file)
                
                # Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
                df['Date'] = pd.to_datetime(df['Date'])
                
                # ì¢…ëª©ëª… ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                filename = csv_file.name
                stock_name = filename.split('_')[0]  # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ ì¢…ëª©ëª…
                
                # í˜„ì¬ ë‚ ì§œ (ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
                latest_date = df['Date'].max()
                
                # ì „ì¼ ë°ì´í„°
                yesterday_data = df.iloc[-2] if len(df) > 1 else df.iloc[-1]
                
                # ì§€ë‚œ ì¼ì£¼ì¼ ë°ì´í„° (ìµœê·¼ 7ì¼)
                week_ago = latest_date - timedelta(days=7)
                week_data = df[df['Date'] >= week_ago]
                
                # ì§€ë‚œ 3ê°œì›” ë°ì´í„° (ìµœê·¼ 90ì¼)
                three_months_ago = latest_date - timedelta(days=90)
                three_months_data = df[df['Date'] >= three_months_ago]
                
                # ì§€ë‚œ 6ê°œì›” ë°ì´í„° (ì „ì²´ ë°ì´í„°)
                six_months_data = df
                
                # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
                summary_parts = []
                summary_parts.append(f"ì¢…ëª©: {stock_name}")
                summary_parts.append("")
                
                # ì „ì¼ ë°ì´í„°
                summary_parts.append(f"ì „ì¼ ê³ ê°€: {yesterday_data['High']:,.0f}, ì €ê°€: {yesterday_data['Low']:,.0f}, ì‹œê°€: {yesterday_data['Open']:,.0f}, ì¢…ê°€: {yesterday_data['Close']:,.0f}, ê±°ë˜ëŸ‰: {yesterday_data['Volume']:,.0f}, ë“±ë½ë¥ : {yesterday_data['Change']:.2f}%")
                
                # ì§€ë‚œ ì¼ì£¼ì¼ ë°ì´í„°
                if len(week_data) > 0:
                    week_high = week_data['High'].max()
                    week_low = week_data['Low'].min()
                    week_change = ((week_data['Close'].iloc[-1] - week_data['Open'].iloc[0]) / week_data['Open'].iloc[0]) * 100
                    summary_parts.append(f"ì§€ë‚œ ì¼ì£¼ì¼ ê³ ê°€: {week_high:,.0f}, ì €ê°€: {week_low:,.0f}, ë“±ë½ë¥ : {week_change:.2f}%")
                else:
                    summary_parts.append("ì§€ë‚œ ì¼ì£¼ì¼ ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                
                # ì§€ë‚œ 3ê°œì›” ë°ì´í„°
                if len(three_months_data) > 0:
                    three_months_high = three_months_data['High'].max()
                    three_months_low = three_months_data['Low'].min()
                    three_months_change = ((three_months_data['Close'].iloc[-1] - three_months_data['Open'].iloc[0]) / three_months_data['Open'].iloc[0]) * 100
                    summary_parts.append(f"ì§€ë‚œ 3ê°œì›” ê³ ê°€: {three_months_high:,.0f}, ì €ê°€: {three_months_low:,.0f}, ë“±ë½ë¥ : {three_months_change:.2f}%")
                else:
                    summary_parts.append("ì§€ë‚œ 3ê°œì›” ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                
                # ì§€ë‚œ 6ê°œì›” ë°ì´í„°
                if len(six_months_data) > 0:
                    six_months_high = six_months_data['High'].max()
                    six_months_low = six_months_data['Low'].min()
                    six_months_change = ((six_months_data['Close'].iloc[-1] - six_months_data['Open'].iloc[0]) / six_months_data['Open'].iloc[0]) * 100
                    summary_parts.append(f"ì§€ë‚œ 6ê°œì›” ê³ ê°€: {six_months_high:,.0f}, ì €ê°€: {six_months_low:,.0f}, ë“±ë½ë¥ : {six_months_change:.2f}%")
                else:
                    summary_parts.append("ì§€ë‚œ 6ê°œì›” ê³ ê°€: N/A, ì €ê°€: N/A, ë“±ë½ë¥ : N/A")
                
                return "\n".join(summary_parts)
                
            except Exception as e:
                print(f"    âŒ CSV ìš”ì•½ ë³€í™˜ ì‹¤íŒ¨: {csv_file.name} - {e}")
                return f"íŒŒì¼ëª…: {csv_file.name} (ìš”ì•½ ë³€í™˜ ì‹¤íŒ¨)"

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        vector_manager = MetadataOnlyVectorManager(
            data_dir="/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1"
        )
        
        success = vector_manager.process_documents()
        
        if success:
            print("\nâœ… ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            print("ğŸ“Š ë‰´ìŠ¤ ë°ì´í„°ê°€ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤!")
            
            # ì‹¤ì œ ë©”íƒ€ë°ì´í„° ì €ì¥
            print("\nğŸ’¾ ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°ë¥¼ vector_db_1ì— ì €ì¥í•©ë‹ˆë‹¤...")
            
            # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ìƒì„± (process_documentsì—ì„œ ìƒì„±ëœ ê²ƒ ì‚¬ìš©)
            new_metadata = []
            
            # CSV íŒŒì¼ ì²˜ë¦¬ - ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜
            csv_files = list(Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1").glob("*.csv"))
            print(f"  ğŸ“Š CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
            for csv_file in csv_files:
                try:
                    print(f"    ğŸ“Š CSV íŒŒì¼ ì²˜ë¦¬: {csv_file.name}")
                    # ìš”ì•½ëœ í˜•íƒœë¡œ ë³€í™˜
                    summary_text = vector_manager._csv_to_summary_text(csv_file)
                    print(f"      ğŸ“ ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(summary_text)}ì")
                    print(f"      ğŸ“ ìš”ì•½ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {summary_text[:200]}...")
                    
                    # ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹ (ë³´í†µ 1ê°œ ì²­í¬ê°€ ë  ê²ƒ)
                    chunks = vector_manager._segment_text_with_clova(summary_text, max_length=2048)
                    print(f"      ğŸ“ ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                    
                    for i, chunk in enumerate(chunks):
                        metadata_entry = {
                            "filename": csv_file.name,
                            "type": "csv",
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "text_content": chunk,
                            "text_length": len(chunk),
                            "created_at": "2025-01-31T00:00:00"
                        }
                        new_metadata.append(metadata_entry)
                        
                except Exception as e:
                    print(f"  âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # JSON íŒŒì¼ ì²˜ë¦¬ (ë‰´ìŠ¤ ë°ì´í„°) - ê°œë³„ ë‰´ìŠ¤ë¡œ ì²˜ë¦¬
            json_files = list(Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1").glob("*.json"))
            print(f"  ğŸ“„ JSON íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
            for json_file in json_files:
                try:
                    print(f"    ğŸ” JSON íŒŒì¼ ì²˜ë¦¬: {json_file.name}")
                    with open(json_file, 'r', encoding='utf-8') as f:
                        news_data = json.load(f)
                    
                    # ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ì²˜ë¦¬
                    articles = []
                    stocks = news_data.get('stocks', {})
                    for stock_name, stock_data in stocks.items():
                        stock_articles = stock_data.get('items', [])
                        for article in stock_articles:
                            article['stock_name'] = stock_name  # ì¢…ëª©ëª… ì¶”ê°€
                            articles.append(article)
                    
                    print(f"      ğŸ“° ì´ {len(articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ë°œê²¬")
                    for article_index, article in enumerate(articles):
                        print(f"        ğŸ“° ê¸°ì‚¬ {article_index+1} ì²˜ë¦¬ ì¤‘...")
                        # ê°œë³„ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        article_text = vector_manager._single_article_to_text(article, article_index)
                        
                        # ì²­í‚¹
                        chunks = vector_manager._segment_text_with_clova(article_text, max_length=1024)
                        print(f"        ğŸ“ ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬")
                        
                        # ê° ì²­í¬ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ìƒì„±
                        for chunk_index, chunk in enumerate(chunks):
                            metadata_entry = {
                                "filename": json_file.name,
                                "type": "news",
                                "article_index": article_index,
                                "chunk_index": chunk_index,
                                "total_articles": len(articles),
                                "total_chunks": len(chunks),
                                "title": article.get('title', ''),
                                "text_content": chunk,
                                "text_length": len(chunk),
                                "created_at": "2025-01-31T00:00:00"
                            }
                            new_metadata.append(metadata_entry)
                            print(f"          ğŸ“ ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€: article_index={article_index}, chunk_index={chunk_index}")
                        
                except Exception as e:
                    print(f"  âŒ JSON íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
            metadata_file = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/vector_db_1/hybrid_metadata.json")
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(new_metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")
            print(f"ğŸ“Š ì´ {len(new_metadata)}ê°œ ë©”íƒ€ë°ì´í„° í•­ëª© ì €ì¥ë¨")
            
            # ì €ì¥ëœ ë©”íƒ€ë°ì´í„° í™•ì¸
            print(f"\nğŸ” ì €ì¥ëœ ë©”íƒ€ë°ì´í„° í™•ì¸:")
            csv_count = len([m for m in new_metadata if m["type"] == "csv"])
            news_count = len([m for m in new_metadata if m["type"] == "news"])
            print(f"  ğŸ“Š CSV í•­ëª©: {csv_count}ê°œ")
            print(f"  ğŸ“° ë‰´ìŠ¤ í•­ëª©: {news_count}ê°œ")
            
            if news_count > 0:
                print("âœ… ë‰´ìŠ¤ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
        else:
            print("\nâŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_metadata_generation() 