#!/usr/bin/env python3
"""
ë°ì´í„° ë¶„ì„ ë° CLOVA í†µí•© ì‹œìŠ¤í…œ
- KRX ì¼ì¼ ì£¼ê°€ ë°ì´í„° ë° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CLOVA Segmentationê³¼ Embeddingìœ¼ë¡œ ì²˜ë¦¬
- CLOVA ëª¨ë¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì´ìŠˆ ì¢…ëª© ì¶”ì¶œ
"""

import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import os
from dotenv import load_dotenv

# CLOVA API í´ë¼ì´ì–¸íŠ¸ë“¤
from clova_segmentation import ClovaSegmentationClient
from clova_embedding import ClovaEmbeddingAPI
from clova_chat_client import ClovaChatClient

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
env_path = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/code/.env")
load_dotenv(env_path)

class DataAnalyzer:
    """ë°ì´í„° ë¶„ì„ ë° CLOVA í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data")
        self.output_dir = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data_1")
        self.output_dir.mkdir(exist_ok=True)
        
        # CLOVA API í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”
        self.segmentation_client = ClovaSegmentationClient()
        self.embedding_client = ClovaEmbeddingAPI()
        self.chat_client = ClovaChatClient()
        
        print("ğŸ”§ DataAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
    
    def load_krx_data(self) -> Optional[pd.DataFrame]:
        """KRX ì¼ì¼ ì£¼ê°€ ë°ì´í„° ë¡œë“œ"""
        try:
            # ê°€ì¥ ìµœê·¼ KRX CSV íŒŒì¼ ì°¾ê¸°
            krx_files = list(self.data_dir.glob("krx_daily_trading_*.csv"))
            if not krx_files:
                print("âŒ KRX ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            latest_file = max(krx_files, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“Š KRX ë°ì´í„° íŒŒì¼ ë¡œë“œ: {latest_file.name}")
            
            df = pd.read_csv(latest_file, encoding='utf-8-sig')
            print(f"âœ… KRX ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ì¢…ëª©")
            return df
            
        except Exception as e:
            print(f"âŒ KRX ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_news_data(self) -> Optional[Dict]:
        """ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        try:
            # ê°€ì¥ ìµœê·¼ ë‰´ìŠ¤ JSON íŒŒì¼ ì°¾ê¸°
            news_files = list(self.data_dir.glob("naver_news_*.json"))
            if not news_files:
                print("âŒ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            latest_file = max(news_files, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“° ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ë¡œë“œ: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
            
            print(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(news_data.get('items', []))}ê°œ ê¸°ì‚¬")
            return news_data
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def prepare_krx_text(self, df: pd.DataFrame) -> str:
        """KRX ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        text_parts.append("=== KRX ì¼ì¼ ì£¼ê°€ ë°ì´í„° ===")
        text_parts.append(f"ì´ ì¢…ëª© ìˆ˜: {len(df)}ê°œ")
        text_parts.append(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        text_parts.append("")
        
        # ìƒìœ„ ì¢…ëª©ë“¤ì˜ ì •ë³´ (ê±°ë˜ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬)
        df_sorted = df.sort_values('ACC_TRDVOL', ascending=False)
        
        # ìƒìœ„ 20ê°œ ì¢…ëª©ë§Œ í¬í•¨ (ì„ë² ë”© í•œë„ ê³ ë ¤)
        for idx, row in df_sorted.head(20).iterrows():
            stock_info = []
            stock_info.append(f"ì¢…ëª© {idx+1}: {row.get('ISU_ABBRV', 'N/A')}")
            stock_info.append(f"  ì¢…ëª©ì½”ë“œ: {row.get('ISU_CD', 'N/A')}")
            stock_info.append(f"  ì¢…ê°€: {row.get('TDD_CLSPRC', 'N/A')}ì›")
            stock_info.append(f"  ì „ì¼ëŒ€ë¹„: {row.get('CMPPREVDD_PRC', 'N/A')}ì› ({row.get('FLUC_RT', 'N/A')}%)")
            stock_info.append(f"  ê±°ë˜ëŸ‰: {row.get('ACC_TRDVOL', 'N/A')}ì£¼")
            stock_info.append(f"  ê±°ë˜ëŒ€ê¸ˆ: {row.get('ACC_TRDVAL', 'N/A')}ì›")
            stock_info.append(f"  ì‹œê°€ì´ì•¡: {row.get('MKTCAP', 'N/A')}ì›")
            stock_info.append("")
            
            text_parts.extend(stock_info)
        
        return "\n".join(text_parts)
    
    def prepare_news_text(self, news_data: Dict) -> str:
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        text_parts.append("=== ìµœì‹  ë‰´ìŠ¤ ë°ì´í„° ===")
        text_parts.append(f"ì´ ë‰´ìŠ¤ ìˆ˜: {len(news_data.get('items', []))}ê°œ")
        text_parts.append(f"ê²€ìƒ‰ì–´: {news_data.get('query', 'N/A')}")
        text_parts.append("")
        
        # ë‰´ìŠ¤ ê¸°ì‚¬ë“¤
        for idx, item in enumerate(news_data.get('items', []), 1):
            news_info = []
            news_info.append(f"ë‰´ìŠ¤ {idx}:")
            news_info.append(f"  ì œëª©: {item.get('title', 'N/A')}")
            news_info.append(f"  ì„¤ëª…: {item.get('description', 'N/A')}")
            news_info.append(f"  ë°œí–‰ì¼: {item.get('pubDate', 'N/A')}")
            news_info.append(f"  ë§í¬: {item.get('link', 'N/A')}")
            news_info.append("")
            
            text_parts.extend(news_info)
        
        return "\n".join(text_parts)
    
    def segment_data(self, text: str) -> List[str]:
        """CLOVA Segmentationìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        try:
            print("ğŸ”ª CLOVA Segmentationìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
            chunks = self.segmentation_client.segment_text(
                text=text,
                max_length=512,
                overlap=50
            )
            
            if chunks:
                print(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
                return chunks
            else:
                print("âŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨")
                return [text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
                
        except Exception as e:
            print(f"âŒ Segmentation ì˜¤ë¥˜: {e}")
            return [text]
    
    def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """CLOVA Embeddingìœ¼ë¡œ ë²¡í„° ìƒì„±"""
        try:
            print("ğŸ”¢ CLOVA Embeddingìœ¼ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            embeddings = self.embedding_client.get_text_embeddings(texts)
            
            if embeddings:
                print(f"âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
                return embeddings
            else:
                print("âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                return []
                
        except Exception as e:
            print(f"âŒ Embedding ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_with_clova(self, krx_text: str, news_text: str) -> Optional[str]:
        """CLOVA ëª¨ë¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì´ìŠˆ ì¢…ëª© ë¶„ì„"""
        try:
            print("ğŸ¤– CLOVA ëª¨ë¸ì—ê²Œ ë°ì´í„° ì „ë‹¬ ì¤‘...")
            
            # ë¶„ì„ ìš”ì²­ ë©”ì‹œì§€ êµ¬ì„±
            analysis_prompt = f"""
ë‹¤ìŒì€ ì§€ë‚œ 24ì‹œê°„ì˜ êµ­ë‚´ ì£¼ì‹ ì‹œì¥ ë°ì´í„°ì…ë‹ˆë‹¤.

=== ì£¼ê°€ ë°ì´í„° ===
{krx_text}

=== ë‰´ìŠ¤ ë°ì´í„° ===
{news_text}

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ì´ìŠˆê°€ ë˜ì—ˆë˜ ì£¼ì‹ ì¢…ëª©ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ê° ì¢…ëª©ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì¢…ëª©ëª…
2. ì´ìŠˆê°€ ëœ ì´ìœ  (ë‰´ìŠ¤ ë‚´ìš© ê¸°ë°˜)
3. ì£¼ê°€ ë³€ë™ ìƒí™©
4. í–¥í›„ ì „ë§

ë¶„ì„ ê²°ê³¼ëŠ” JSON í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
            
            # CLOVA Chat APIë¡œ ë¶„ì„ ìš”ì²­
            response = self.chat_client.create_chat_completion(
                messages=[{"role": "user", "content": analysis_prompt}],
                max_tokens=1000,
                temperature=0.7
            )
            
            if response:
                print("âœ… CLOVA ë¶„ì„ ì™„ë£Œ")
                print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì")
                return response
            else:
                print("âŒ CLOVA ë¶„ì„ ì‹¤íŒ¨ - ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return None
                
        except Exception as e:
            print(f"âŒ CLOVA ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def save_analysis_result(self, result: str, filename: str = None) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"clova_analysis_{timestamp}.json"
        
        file_path = self.output_dir / filename
        
        try:
            # JSON í˜•íƒœë¡œ ì €ì¥
            analysis_data = {
                "analysis_time": datetime.now().isoformat(),
                "result": result
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return str(file_path)
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def run_analysis(self) -> bool:
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ ë°ì´í„° ë¶„ì„ ë° CLOVA í†µí•© ì‹œìŠ¤í…œ")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
        krx_df = self.load_krx_data()
        news_data = self.load_news_data()
        
        if krx_df is None or news_data is None:
            print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # 2. í…ìŠ¤íŠ¸ ë³€í™˜
        print("\n2ï¸âƒ£ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
        krx_text = self.prepare_krx_text(krx_df)
        news_text = self.prepare_news_text(news_data)
        
        # 3. ë‰´ìŠ¤ ë°ì´í„° ë¨¼ì € Segmentation ë° Embedding (í•œë„ ìš°ì„  ì²˜ë¦¬)
        print("\n3ï¸âƒ£ ë‰´ìŠ¤ ë°ì´í„° CLOVA Segmentation ì¤‘...")
        news_chunks = self.segment_data(news_text)
        
        print("\n4ï¸âƒ£ ë‰´ìŠ¤ ë°ì´í„° CLOVA Embedding ì¤‘...")
        news_embeddings = self.create_embeddings(news_chunks)
        
        # 4. KRX ë°ì´í„° Segmentation ë° Embedding
        print("\n5ï¸âƒ£ KRX ë°ì´í„° CLOVA Segmentation ì¤‘...")
        krx_chunks = self.segment_data(krx_text)
        
        print("\n6ï¸âƒ£ KRX ë°ì´í„° CLOVA Embedding ì¤‘...")
        krx_embeddings = self.create_embeddings(krx_chunks)
        
        # 5. CLOVA ë¶„ì„
        print("\n7ï¸âƒ£ CLOVA ëª¨ë¸ ë¶„ì„ ì¤‘...")
        analysis_result = self.analyze_with_clova(krx_text, news_text)
        
        if analysis_result:
            # 6. ê²°ê³¼ ì €ì¥
            print("\n8ï¸âƒ£ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
            saved_path = self.save_analysis_result(analysis_result)
            
            if saved_path:
                print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
                print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {saved_path}")
                return True
        
        print("\nâŒ ë¶„ì„ ì‹¤íŒ¨")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = DataAnalyzer()
    success = analyzer.run_analysis()
    
    if success:
        print("\nâœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    else:
        print("\nâŒ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main() 