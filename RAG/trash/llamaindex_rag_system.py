#!/usr/bin/env python3
"""
LlamaIndex ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (CLOVA API + í•œêµ­ì–´ LLaMA)
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from dotenv import load_dotenv

# LlamaIndex ê´€ë ¨ import
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Document,
    Settings,
    StorageContext,
    load_index_from_storage
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever

# CLOVA API í´ë¼ì´ì–¸íŠ¸ë“¤
from clova_chat_client import ClovaChatClient
from clova_embedding import ClovaEmbeddingAPI
from clova_segmentation import ClovaSegmentationClient

# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
import numpy as np
import http.client
import json

class LlamaIndexRAGSystem:
    """LlamaIndex ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_dir: str = "/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/data"):
        self.data_dir = Path(data_dir)
        self.index = None
        self.retriever = None
        self.storage_context = None
        
        # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        # ì„¤ì • ì´ˆê¸°í™”
        self._setup_embedding()
        self._setup_chat_client()
        self._setup_segmentation()
        
    def _setup_chat_client(self):
        """HyperCLOVA X Chat API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        print("HyperCLOVA X Chat API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì¤‘...")
        
        self.chat_client = ClovaChatClient()
        
        # API ì •ë³´ í™•ì¸
        api_info = self.chat_client.get_api_info()
        if not api_info["api_key_set"]:
            print("ê²½ê³ : CLOVA API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        if not api_info["request_id_set"]:
            print("ê²½ê³ : CLOVA Request IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
    def _setup_embedding(self):
        """CLOVA X Embedding API ì„¤ì •"""
        print("CLOVA X Embedding API ì„¤ì • ì¤‘...")
        
        self.embed_model = ClovaEmbeddingAPI()
        
        # ì „ì—­ ì„¤ì •
        Settings.embed_model = self.embed_model
    
    def _setup_segmentation(self):
        """CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ API ì„¤ì •"""
        print("CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ API ì„¤ì • ì¤‘...")
        
        self.segmentation_client = ClovaSegmentationClient()
        
        # API ì •ë³´ í™•ì¸
        api_info = self.segmentation_client.get_api_info()
        if not api_info["api_key_set"]:
            print("ê²½ê³ : CLOVA API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        if not api_info["request_id_set"]:
            print("ê²½ê³ : CLOVA Request IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    

        
    def _load_documents(self) -> List[Document]:
        """ëª¨ë“  ë°ì´í„° íŒŒì¼ë“¤ì„ LlamaIndex Documentë¡œ ë³€í™˜"""
        documents = []
        
        if not self.data_dir.exists():
            print(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_dir}")
            return documents
        
        # CSV íŒŒì¼ ì²˜ë¦¬
        csv_documents = self._csv_to_documents()
        documents.extend(csv_documents)
        
        # ë‰´ìŠ¤ ë³¸ë¬¸ JSON íŒŒì¼ ì²˜ë¦¬
        news_documents = self._news_articles_to_documents()
        documents.extend(news_documents)
        
        return documents
    
    def _csv_to_documents(self) -> List[Document]:
        """CSV íŒŒì¼ë“¤ì„ LlamaIndex Documentë¡œ ë³€í™˜"""
        documents = []
        
        csv_files = list(self.data_dir.glob("*.csv"))
        
        if not csv_files:
            print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return documents
            
        print(f"ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
        
        for csv_file in csv_files:
            try:
                print(f"ì²˜ë¦¬ ì¤‘: {csv_file.name}")
                
                # CSV íŒŒì¼ ì½ê¸°
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                # DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                text_content = self._dataframe_to_text(df, csv_file.stem)
                
                # Document ìƒì„±
                doc = Document(
                    text=text_content,
                    metadata={
                        "source": str(csv_file),
                        "filename": csv_file.name,
                        "type": "csv",
                        "rows": len(df),
                        "columns": list(df.columns)
                    }
                )
                
                documents.append(doc)
                print(f"  - {csv_file.name}: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
                
            except Exception as e:
                print(f"ì˜¤ë¥˜: {csv_file} ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")
                
        return documents
    
    def _dataframe_to_text(self, df: pd.DataFrame, filename: str) -> str:
        """DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        # íŒŒì¼ëª… ì •ë³´
        text_parts.append(f"íŒŒì¼ëª…: {filename}")
        text_parts.append(f"ì´ {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        text_parts.append("")
        
        # KRX ë°ì´í„°ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if 'ISU_ABBRV' in df.columns and 'TDD_CLSPRC' in df.columns:
            text_parts.append("=== KRX ì¼ì¼ê±°ë˜ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§) ===")
            text_parts.append("")
            
            # ì „ì²´ ë°ì´í„°ì—ì„œ ê±°ë˜ëŒ€ê¸ˆ í•˜ìœ„ 70%ì™€ ë“±ë½ë¥  í•˜ìœ„ 70%ì—ì„œ ê²¹ì¹˜ëŠ” ì£¼ì‹ ì œì™¸
            df_processed = df.copy()
            
            # FLUC_RT ì»¬ëŸ¼ì„ ë“±ë½ë¥ ë¡œ ì‚¬ìš©
            df_processed['ë“±ë½ë¥ '] = df_processed['FLUC_RT']
            df_processed['ë“±ë½ë¥ _ì ˆëŒ€ê°’'] = df_processed['ë“±ë½ë¥ '].abs()
            
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df_processed)}ê°œ ì¢…ëª©")
            
            # ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸°
            df_trading_value_sorted = df_processed.sort_values('ACC_TRDVAL', ascending=True)
            bottom_70_percent_trading = int(len(df_trading_value_sorted) * 0.7)
            low_trading_stocks = set(df_trading_value_sorted.head(bottom_70_percent_trading)['ISU_ABBRV'].tolist())
            
            # ë“±ë½ë¥  ê¸°ì¤€ í•˜ìœ„ 70% ì°¾ê¸° (ì ˆëŒ€ê°’ ê¸°ì¤€)
            df_change_rate_sorted = df_processed.sort_values('ë“±ë½ë¥ _ì ˆëŒ€ê°’', ascending=True)
            bottom_70_percent_change = int(len(df_change_rate_sorted) * 0.7)
            low_change_stocks = set(df_change_rate_sorted.head(bottom_70_percent_change)['ISU_ABBRV'].tolist())
            
            # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ ì°¾ê¸°
            overlapping_stocks = low_trading_stocks.intersection(low_change_stocks)
            
            print(f"ğŸ“Š ê±°ë˜ëŒ€ê¸ˆ í•˜ìœ„ 70%: {len(low_trading_stocks)}ê°œ ì¢…ëª©")
            print(f"ğŸ“Š ë“±ë½ë¥  í•˜ìœ„ 70%: {len(low_change_stocks)}ê°œ ì¢…ëª©")
            print(f"ğŸ“Š ê²¹ì¹˜ëŠ” ì¢…ëª©: {len(overlapping_stocks)}ê°œ")
            
            # ê²¹ì¹˜ëŠ” ì£¼ì‹ë“¤ì„ ì œì™¸í•œ ìµœì¢… í•„í„°ë§
            df_final_filtered = df_processed[~df_processed['ISU_ABBRV'].isin(overlapping_stocks)]
            
            print(f"ğŸ“Š ìµœì¢… í•„í„°ë§ ê²°ê³¼: {len(df_processed)}ê°œ â†’ {len(df_final_filtered)}ê°œ")
            
            # ê° ì¢…ëª©ë³„ë¡œ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
            for idx, row in df_final_filtered.iterrows():
                stock_name = row.get('ISU_ABBRV', '')  # ì‹¤ì œ ì»¬ëŸ¼ëª…
                stock_code = row.get('ISU_CD', '')     # ì‹¤ì œ ì»¬ëŸ¼ëª…
                
                if stock_name and stock_code:
                    # ê°€ê²© ì •ë³´
                    open_price = row.get('TDD_OPNPRC', 0)
                    high_price = row.get('TDD_HGPRC', 0)
                    low_price = row.get('TDD_LWPRC', 0)
                    close_price = row.get('TDD_CLSPRC', 0)
                    trading_value = row.get('ACC_TRDVAL', 0)
                    change_rate = row.get('ë“±ë½ë¥ ', 0.0)
                    
                    # ì¢…ëª© ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ êµ¬ì„±
                    stock_info = f"ì¢…ëª©ëª…: {stock_name}, ì‹œê°€: {open_price}, ê³ ê°€: {high_price}, ì €ê°€: {low_price}, ì¢…ê°€: {close_price}, ê±°ë˜ëŒ€ê¸ˆ: {trading_value}, ë“±ë½ë¥ : {change_rate:.2f}"
                    text_parts.append(stock_info)
            
            text_parts.append("")
            text_parts.append(f"ì´ {len(df_final_filtered)}ê°œ ì¢…ëª©ì˜ ê±°ë˜ ì •ë³´ (70% ê¸°ì¤€ í•„í„°ë§)")
        else:
            # ì¼ë°˜ CSV íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            text_parts.append("ì»¬ëŸ¼ëª…:")
            for col in df.columns:
                text_parts.append(f"  - {col}")
            text_parts.append("")
            
            # ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰)
            text_parts.append("ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰):")
            sample_df = df.head(10)
            text_parts.append(sample_df.to_string(index=False))
        
        return "\n".join(text_parts)
    
    def _news_articles_to_documents(self) -> List[Document]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ LlamaIndex Documentë¡œ ë³€í™˜"""
        documents = []
        
        # naver_news_*.json íŒŒì¼ë“¤ ì°¾ê¸°
        news_files = list(self.data_dir.glob("naver_news_*.json"))
        
        if not news_files:
            print(f"ë‰´ìŠ¤ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return documents
            
        print(f"ë°œê²¬ëœ ë‰´ìŠ¤ íŒŒì¼: {len(news_files)}ê°œ")
        
        for news_file in news_files:
            try:
                print(f"ì²˜ë¦¬ ì¤‘: {news_file.name}")
                
                # JSON íŒŒì¼ ì½ê¸°
                with open(news_file, 'r', encoding='utf-8') as f:
                    news_data = json.load(f)
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
                articles = []
                if 'items' in news_data:
                    articles = news_data['items']
                elif 'articles' in news_data:
                    articles = news_data['articles']
                
                print(f"  - {len(articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ì²˜ë¦¬")
                
                # ê° ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ Documentë¡œ ë³€í™˜
                for i, article in enumerate(articles):
                    text_content = self._article_to_text(article, i+1)
                    
                    doc = Document(
                        text=text_content,
                        metadata={
                            "source": str(news_file),
                            "filename": news_file.name,
                            "type": "news_article",
                            "article_index": i+1,
                            "title": article.get('title', ''),
                            "url": article.get('link', ''),
                            "publish_date": article.get('pubDate', '')
                        }
                    )
                    documents.append(doc)
                
            except Exception as e:
                print(f"ì˜¤ë¥˜: {news_file} ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")
                
        return documents
    
    def _article_to_text(self, article: Dict, index: int) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        # ê¸°ì‚¬ ì œëª©
        title = article.get('title', '')
        if title:
            text_parts.append(f"ì œëª©: {title}")
        
        # ê¸°ì‚¬ ë‚´ìš© (ì„¤ëª…)
        description = article.get('description', '')
        if description:
            # HTML íƒœê·¸ ì œê±°
            import re
            clean_description = re.sub(r'<[^>]+>', '', description)
            text_parts.append(f"ë‚´ìš©: {clean_description}")
        
        # ë°œí–‰ì¼
        pub_date = article.get('pubDate', '')
        if pub_date:
            text_parts.append(f"ë°œí–‰ì¼: {pub_date}")
        
        return "\n".join(text_parts)
    

        
    def build_index(self, rebuild: bool = False) -> bool:
        """ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            # ì ˆëŒ€ ê²½ë¡œë¡œ vector_db ë””ë ‰í† ë¦¬ ì„¤ì •
            storage_path = Path("/Users/Chris/Desktop/JH/MiraeassetNaver/RAG/vector_db")
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆê³  rebuildê°€ Falseì¸ ê²½ìš° ë¡œë“œ
            if not rebuild and storage_path.exists():
                print("ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.storage_context = StorageContext.from_defaults(persist_dir=str(storage_path))
                self.index = load_index_from_storage(self.storage_context)
                print("ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
                return True
            
            print("ìƒˆë¡œìš´ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            
            # vector_store ë””ë ‰í† ë¦¬ ìƒì„±
            storage_path.mkdir(exist_ok=True)
            
            # ëª¨ë“  ë°ì´í„° íŒŒì¼ë“¤ì„ Documentë¡œ ë³€í™˜
            documents = self._load_documents()
            
            if not documents:
                print("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
                return False
            
            print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
            # ë¬¸ì„œ ì²­í‚¹
            print("ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
            chunked_documents = []
            
            for doc in documents:
                # ë¬¸ì„œ í¬ê¸° ë¡œê¹…
                print(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {doc.metadata.get('filename', 'unknown')} ({len(doc.text):,}ì)")
                
                # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •
                doc_type = doc.metadata.get('type', 'unknown')
                
                if doc_type == 'csv':
                    # KRX CSV ë°ì´í„°: ìµœì í™”ëœ ì„¤ì • ì‚¬ìš©
                    print(f"  ğŸ”§ CSV ë°ì´í„° - ìµœì í™”ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©")
                    chunks = self.segmentation_client.segment_text(
                        doc.text, 
                        max_length=2048  # í° ì²­í¬ í¬ê¸°
                    )
                else:
                    # ë‰´ìŠ¤ ë°ì´í„°: ì›ë˜ ì„¤ì • ì‚¬ìš©
                    print(f"  ğŸ“° ë‰´ìŠ¤ ë°ì´í„° - ê¸°ë³¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©")
                    chunks = self.segmentation_client.segment_text(
                        doc.text, 
                        max_length=512  # ì‘ì€ ì²­í¬ í¬ê¸° (ë‰´ìŠ¤ì— ì í•©)
                    )
                
                if chunks:
                    # ê° ì²­í¬ë¥¼ ë³„ë„ì˜ Documentë¡œ ìƒì„±
                    for i, chunk in enumerate(chunks):
                        chunk_doc = Document(
                            text=chunk,
                            metadata={
                                **doc.metadata,
                                "chunk_index": i,
                                "total_chunks": len(chunks),
                                "original_document": doc.metadata.get("filename", "unknown")
                            }
                        )
                        chunked_documents.append(chunk_doc)
                else:
                    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë¬¸ì„œ ì‚¬ìš©
                    print(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨, ì›ë³¸ ë¬¸ì„œ ì‚¬ìš©: {doc.metadata.get('filename', 'unknown')}")
                    chunked_documents.append(doc)
            
            print(f"ì´ {len(chunked_documents)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
            
            # ì„ë² ë”© ì§„í–‰ìƒí™© ì¶”ì ì„ ìœ„í•œ ì´ ìš”ì²­ ìˆ˜ ì„¤ì •
            if hasattr(self.embed_model, '_embedding_executor'):
                self.embed_model._embedding_executor._total_requests = len(chunked_documents)
                self.embed_model._embedding_executor._completed_requests = 0
                print(f"ğŸ“Š ì„ë² ë”© ì§„í–‰ìƒí™© ì¶”ì  ì„¤ì •: ì´ {len(chunked_documents)}ê°œ ì²­í¬")
            
            # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (ì²­í‚¹ëœ ë¬¸ì„œë“¤ ì‚¬ìš©)
            print("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            try:
                self.index = VectorStoreIndex.from_documents(
                    chunked_documents,
                    show_progress=True
                )
                print("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                print(f"ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            # ì¸ë±ìŠ¤ ì €ì¥
            print("ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
            try:
                # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ì˜ ê²½ìš° ì§ì ‘ ì €ì¥
                self.index.storage_context.persist(persist_dir=str(storage_path))
                print("ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            except Exception as e:
                print(f"ì¸ë±ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                # ì €ì¥ ì‹¤íŒ¨í•´ë„ ì¸ë±ìŠ¤ëŠ” ì‚¬ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ ê³„ì† ì§„í–‰
                print("ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            
            print("ì¸ë±ìŠ¤ êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
            
    def setup_retriever(self) -> bool:
        """ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •"""
        if self.index is None:
            print("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return False
            
        try:
            # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
            self.retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=5
            )
            
            print("ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
            
    def ask(self, query: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        if self.retriever is None:
            return {
                "success": False,
                "error": "ë¦¬íŠ¸ë¦¬ë²„ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
            
        try:
            print(f"ì§ˆë¬¸: {query}")
            print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_nodes = self.retriever.retrieve(query)
            
            if not retrieved_nodes:
                return {
                    "success": False,
                    "error": "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "query": query
                }
            
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved_nodes)}ê°œ")
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ HyperCLOVA Xì— ì „ë‹¬
            print("HyperCLOVA Xë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
            context_documents = []
            for node in retrieved_nodes:
                context_documents.append({
                    "content": node.text,
                    "metadata": node.metadata,
                    "score": node.score if hasattr(node, 'score') else None
                })
            
            # HyperCLOVA Xë¡œ ë‹µë³€ ìƒì„±
            answer = self.chat_client.create_rag_response(query, context_documents)
            
            if answer:
                return {
                    "success": True,
                    "query": query,
                    "answer": answer,
                    "source_nodes": context_documents
                }
            else:
                return {
                    "success": False,
                    "error": "HyperCLOVA X ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                    "query": query
                }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}",
                "query": query
            }
    
    def ask_with_vectors(self, query: str) -> Dict[str, Any]:
        """ì„ë² ë”© ë²¡í„°ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€"""
        if self.index is None:
            return {
                "success": False,
                "error": "ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
            
        try:
            print(f"ì§ˆë¬¸: {query}")
            print("ì„ë² ë”© ë²¡í„° ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            
            # ëª¨ë“  ë…¸ë“œì˜ ì„ë² ë”© ë²¡í„° ìˆ˜ì§‘
            all_nodes = list(self.index.docstore.docs.values())
            embedding_vectors = []
            vector_metadata = []
            
            print(f"ì´ {len(all_nodes)}ê°œì˜ ë…¸ë“œì—ì„œ ë²¡í„° ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            
            for i, node in enumerate(all_nodes):
                try:
                    # ë…¸ë“œì˜ ì„ë² ë”© ë²¡í„° ê°€ì ¸ì˜¤ê¸°
                    if hasattr(node, 'embedding') and node.embedding is not None:
                        embedding_vectors.append(node.embedding)
                        vector_metadata.append({
                            "filename": node.metadata.get('filename', 'Unknown'),
                            "type": node.metadata.get('type', 'Unknown'),
                            "node_index": i,
                            "vector_dimension": len(node.embedding)
                        })
                    else:
                        print(f"ë…¸ë“œ {i}ì— ì„ë² ë”© ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"ë…¸ë“œ {i} ë²¡í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            
            if not embedding_vectors:
                return {
                    "success": False,
                    "error": "ì„ë² ë”© ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "query": query
                }
            
            print(f"ìˆ˜ì§‘ëœ ë²¡í„°: {len(embedding_vectors)}ê°œ")
            print(f"ë²¡í„° ì°¨ì›: {len(embedding_vectors[0])}ì°¨ì›")
            
            # ì„ë² ë”© ë²¡í„°ë¥¼ HyperCLOVA Xì— ì „ë‹¬
            print("HyperCLOVA Xë¡œ ë²¡í„° ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘...")
            
            answer = self.chat_client.create_vector_based_response(query, embedding_vectors, vector_metadata)
            
            if answer:
                return {
                    "success": True,
                    "query": query,
                    "answer": answer,
                    "vector_count": len(embedding_vectors),
                    "vector_dimension": len(embedding_vectors[0]) if embedding_vectors else 0,
                    "vector_metadata": vector_metadata
                }
            else:
                return {
                    "success": False,
                    "error": "HyperCLOVA X ë²¡í„° ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                    "query": query
                }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"ë²¡í„° ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}",
                "query": query
            }
            
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        info = {
            "data_directory": str(self.data_dir),
            "index_built": self.index is not None,
            "retriever_setup": self.retriever is not None,
            "llm_model": "HyperCLOVA X",
            "embedding_model": "CLOVA X Embedding",
            "segmentation_model": "CLOVA Segmentation",
            "clova_api_key_set": bool(os.getenv("CLOVA_API_KEY")),
            "clova_embedding_request_id_set": bool(os.getenv("CLOVA_EMBEDDING_REQUEST_ID")),
            "clova_chat_request_id_set": bool(os.getenv("CLOVA_CHAT_REQUEST_ID"))
        }
        
        if self.index:
            info["index_info"] = {
                "total_nodes": len(self.index.docstore.docs),
                "vector_store_type": type(self.index.vector_store).__name__
            }
            
        return info
        
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ"""
        if self.retriever is None:
            print("ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return
            
        print("\n" + "=" * 50)
        print("LlamaIndex RAG ëŒ€í™”í˜• ì±„íŒ…")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 50)
        
        conversation = []
        
        while True:
            try:
                query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                    
                if not query:
                    continue
                    
                result = self.ask(query)
                
                if result["success"]:
                    print(f"\në‹µë³€: {result['answer']}")
                    
                    # ì†ŒìŠ¤ ì •ë³´ ì¶œë ¥
                    if result.get("source_nodes"):
                        print(f"\nì°¸ê³ í•œ ë¬¸ì„œ: {len(result['source_nodes'])}ê°œ")
                        for i, node in enumerate(result["source_nodes"][:3], 1):
                            print(f"  {i}. {node['metadata'].get('filename', 'Unknown')}")
                            print(f"     ë‚´ìš©: {node['content'][:100]}...")
                    
                    conversation.append({
                        "query": query,
                        "answer": result["answer"],
                        "sources_count": len(result.get("source_nodes", []))
                    })
                else:
                    print(f"\nì˜¤ë¥˜: {result['error']}")
                    
            except KeyboardInterrupt:
                print("\n\nì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        
        # ëŒ€í™” ë‚´ìš© ì €ì¥
        if conversation:
            self._save_conversation(conversation)
            
    def _save_conversation(self, conversation: List[Dict[str, Any]], filename: str = "llamaindex_conversation.json"):
        """ëŒ€í™” ë‚´ìš© ì €ì¥"""
        conversation_path = Path(filename)
        
        with open(conversation_path, 'w', encoding='utf-8') as f:
            json.dump(conversation, f, ensure_ascii=False, indent=2)
        
        print(f"\nëŒ€í™” ë‚´ìš© ì €ì¥ ì™„ë£Œ: {conversation_path}")
        print(f"ì´ {len(conversation)}ê°œì˜ ëŒ€í™”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.") 

 